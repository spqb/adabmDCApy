{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the <code>adabmDCA 2.0</code> Documentation","text":"<p><code>adabmDCA</code> is a versatile library for Direct Coupling Analysis (DCA), enabling the training, sampling, and application of Boltzmann Machines (Potts models) on biological sequence data.</p> <p>Instructons</p> <p>This documentation is meant for providing a user-friendly description of the <code>adabmDCA</code> package main features. It is supported by:</p> <ul> <li>The main article [Rosset et al., 2025], with detailed explanations of the main features. The present documentation is a shorter version of the paper, but it includes additional features</li> <li>The Colab notebook providing a tutorial of the APIs for training, sampling and analyzing a <code>bmDCA</code> model (Python only)</li> </ul> <p>This tutorial introduces the new and enhanced version of <code>adabmDCA</code> [Muntoni at al., 2021]. The software is available in three language-specific implementations:</p> <ul> <li>C++ \u2013 optimized for single-core CPUs  </li> <li>Julia \u2013 ideal for multi-core CPU setups  </li> <li>Python \u2013 GPU-accelerated and feature-rich</li> </ul> <p>All versions share a unified terminal-based interface, allowing users to choose based on their hardware and performance needs.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":""},{"location":"#model-training","title":"\ud83e\udde0 Model Training","text":"<p>Choose from three training strategies to fit your model complexity and goals:</p> <ul> <li><code>bmDCA</code>: Fully-connected Boltzmann Machine [Figliuzzi et al., 2018]</li> <li><code>eaDCA</code>: Sparse model with progressively added couplings [Calvanese et al., 2024]</li> <li><code>edDCA</code>: Prunes an existing <code>bmDCA</code> model down to a sparse network [Barrat-Charlaix et al., 2021]</li> </ul>"},{"location":"#applications-of-pretrained-models","title":"\u2699\ufe0f Applications of Pretrained Models","text":"<p>Once trained, models can be used to:</p> <ul> <li>Generate new sequences</li> <li>Predict structural contacts [Ekeberg et al., 2013].</li> <li>Score sequence datasets based on model energy</li> <li>Build mutational libraries with DCA-based scoring</li> </ul>"},{"location":"#advanced-features-in-python-adabmdcapy","title":"\ud83d\ude80 Advanced Features in Python (<code>adabmDCApy</code>)","text":"<p>The Python version includes exclusive features:</p> <ul> <li>Experimental feedback reintegration for refined models [Calvanese et al., 2025]</li> <li>Thermodynamic integration to estimate model entropy</li> <li><code>Profmark</code>: GPU-accelerated dataset splitting with phylogenetic and sampling bias control, based on the <code>cobalt</code> algorithm [Petti et al., 2022]</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run? Skip ahead to the Quicklist for command-line usage examples.</p>"},{"location":"applications/","title":"Applications","text":"<p>Info</p> <p>We report in the Script arguments section the list of all the possible input arguments of each routine. The same information can be shown from the command line using:</p> <p><code>adabmDCA &lt;routine_name&gt; -h</code></p>"},{"location":"applications/#generate-sequences","title":"\ud83e\uddec Generate Sequences","text":"<p>Once a model is trained, it can be used to generate new sequences with:</p> <pre><code>adabmDCA sample -p &lt;path_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li><code>&lt;output_folder&gt;</code>: directory to save the output.</li> <li><code>&lt;num_gen&gt;</code>: number of sequences to generate.</li> </ul> <p>The tool first estimates the mixing time <code>t_mix</code> by simulating chains from the MSA. It then initializes <code>num_gen</code> Markov chains and runs <code>nmix * t_mix</code> sweeps (default <code>nmix = 2</code>) to ensure thermalization. </p> <p>\ud83d\udce6 Output Files:</p> <ul> <li>A FASTA file of generated sequences</li> <li>A log file for reproducing the mixing time surves (Fig. 3-left)</li> <li>A log file tracking the Pearson \\(C_{ij}\\) score as a function of the sampling time </li> </ul>"},{"location":"applications/#convergence-criterion","title":"Convergence Criterion","text":"<p>To ensure proper sampling, sequence identity is used to track mixing:</p> <ul> <li>\\(\\mathrm{SeqID}(t)\\) = identity between pairs of independent samples</li> <li>\\(\\mathrm{SeqID}(t, t/2)\\) = identity between the same chain at different times</li> </ul> <p>Denoting \\(\\pmb{a}_i(t)\\) the i-th sequence of the MSA at sampling time \\(t\\), these are computed as:</p> \\[     \\mathrm{SeqID}(t) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_{\\sigma(i)}(t)) \\qquad \\mathrm{SeqID}(t, t/2) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_i(t/2)) \\] <p>where \\(\\sigma(i)\\) is a random permutation of the index \\(i\\) and</p> \\[     \\mathrm{SeqID}(\\pmb{a}, \\pmb{b}) = \\frac{1}{L}\\sum_{i=1}^L \\delta_{a_i, b_i} \\in [0, 1] \\] <p>Convergence is assumed when \\(\\mathrm{SeqID}(t) \\cong \\mathrm{SeqID}(t, t/2)\\).</p> <p> Figure 3: Analysis of a bmDCA model. Left: measuring the mixing time of the model using \\(10^4\\) chains. The curves represent the average overlap among randomly initialized samples (dark blue) and the one among the same sequences between times \\(t\\) and \\(t/2\\) (light blue). Shaded areas represent the error of the mean. When the two curves merge, we can assume that the chains at time \\(t\\) forgot the memory of the chains at time \\(t/2\\). This point gives us an estimate of the model's mixing time, \\(t^{\\mathrm{mix}}\\).  Notice that the times start from 1, so the starting conditions are not shown. Right: Scatter plot of the entries of the Covariance matrix of the data versus that of the generated samples.</p>"},{"location":"applications/#contact-prediction","title":"\ud83d\udd17 Contact Prediction","text":"<p>One of the principal applications of the DCA models has been that of predicting a tertiary structure of a protein or RNA domain. In particular, with each pair of sites \\(i\\) and \\(j\\) in the MSA, <code>adabmDCA 2.0</code> computes a contact score that quantifies how likely the two associated positions in the chains are in contact in the three-dimensional structure. Formally, it corresponds to the average-product corrected (APC) Frobenius norms of the coupling matrices [Ekeberg et al., 2013], i.e.</p> \\[ F_{i,j}^{\\rm APC} = F_{i,j} - \\frac{\\sum_{k} F_{i,k} \\sum_{k} F_{k,j}}{\\sum_{kl} F_{k,l}}, \\quad F_{i,j} = \\sqrt{\\sum_{a,b \\neq '-'} J_{i,j}\\left(a, b \\right)^{2}} \\] <p>To compute contact scores:</p> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>Zero-sum gauge and gap symbols are handled internally.</p> <p>\ud83d\udce6 Output Files:</p> <ul> <li><code>&lt;label&gt;_frobenius.txt</code> with scores for each pair.</li> </ul>"},{"location":"applications/#sequence-scoring","title":"\ud83d\udcc9 Sequence Scoring","text":"<p>To score sequences using the DCA energy with a trained model:</p> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence is annotated with its statistical energy. Lower energies correspond to more likely (or better fitting) sequences under the model.</li> </ul>"},{"location":"applications/#single-mutant-library","title":"\ud83e\uddea Single Mutant Library","text":"<p>To simulate a mutational scan around a wild-type sequence:</p> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence represents a single-point mutant, named by mutation and \\(\\Delta E\\) (change in energy). Example:</li> </ul> <pre><code>&gt;G27A | DCAscore: -0.6\n</code></pre> <p>Negative \\(\\Delta E\\) suggests improved fitness.</p>"},{"location":"applications/#reintegrated-dca-model-from-experiments","title":"\ud83d\udd01 Reintegrated DCA Model from Experiments","text":"<p>As described in [Calvanese et al., 2025], it is possible to train a DCA model informed with experimental feedback in order to improve the model's ability of generating functional sequences:</p> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --lambda_ &lt;lambda_value&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>nat_msa</code>: MSA of natural sequences</li> <li><code>reint_msa</code>: MSA of tested sequences</li> <li><code>adj_vector</code>: a text file containing experimental results for the reintegration dataset. Each line of the file should contain <code>+1</code> or <code>-1</code>, where the i-th line corresponds to:<ul> <li><code>1</code> if the i-th sequence of the <code>reint_msa</code> passes the experimental test;</li> <li><code>-1</code> if the i-th sequence does not pass the experimental test;</li> </ul> </li> <li><code>lambda_</code>: reintegration strength (default: 1)</li> <li><code>alphabet</code>: <code>protein</code> or <code>rna</code>, sequence type</li> </ul> <p>\ud83d\udca1 Tip: It is possible to use continuous values from -1 to 1 for the <code>adj_vector</code>, depending on the performance of the sequence in the experiment. Additionally, the <code>lambda_</code> parameter can be fine-tuned to adjust the reintegration strength. If unsure, a good starting point is to use <code>lambda_</code> = 1 and \u00b11 values for the <code>adj_vector</code>.</p>"},{"location":"applications/#traintest-split-for-homologous-sequences","title":"\ud83e\udde0 Train/Test Split for Homologous Sequences","text":"<p>When dealing with a family of homologous sequences, splitting data into training and test sets has to be done carefully. There are two main reasons for this:</p> <ol> <li>Since homology introduces correlations between sequences, a simple random split would yield a test set that closely reproduces the training set on any statistical test,</li> <li>Because some regions of the sequence space are sampled more than others, the test set might contain densely populated clusters of sequences that would bias any type of assessment.</li> </ol> <p>To overcome these issues, we propose a simplified GPU-accelerated version of the <code>cobalt</code> algorithm introduced in [Petti et al., 2022]. The algorithm proceeds in two steps:</p> <ol> <li>A first train/test split is done, such that no sequence in the test set has more than <code>t1</code> fractional sequences identity with any sequences in the training set;</li> <li>The test set is pruned until any two sequences in it have fractional sequence identity that does not exceeds the value <code>t2</code>.</li> </ol> <p>Typical usage:</p> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre> <p>Required:</p> <ul> <li><code>t1</code>: max train/test identity</li> <li><code>t2</code>: max identity within test set</li> <li><code>n_trials</code>: number of trials to find best split</li> <li><code>output_prefix</code>: generates <code>&lt;output_prefix&gt;.train</code> and <code>&lt;output_prefix&gt;.test</code> files</li> <li><code>input_msa</code>: input MSA in FASTA format</li> </ul> <p>Optional:</p> <ul> <li><code>-t3</code>: max train/train identity</li> <li><code>--maxtrain</code>, <code>--maxtest</code>: size limits for train and test sets</li> <li><code>--alphabet</code>: sequence type (<code>protein</code>, <code>rna</code>, <code>dna</code>)</li> <li><code>--seed</code>: random seed (default 42)</li> <li><code>--device</code>: computation device (default <code>cuda</code>)</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":"<p><code>adabmDCA</code> is available in three language-specific implementations:</p> <ul> <li>Python \u2013 optimized for GPU execution  </li> <li>Julia \u2013 designed for multi-core CPU usage  </li> <li>C++ \u2013 lightweight and single-core CPU compatible</li> </ul> <p>Follow the instructions below based on your preferred environment.</p>"},{"location":"installation/#python-gpu-oriented","title":"\ud83d\udd37 Python (GPU-oriented)","text":""},{"location":"installation/#option-1-install-from-pypi-recommended","title":"\ud83d\udd39 Option 1: Install from PyPI (Recommended)","text":"<pre><code>pip install adabmDCA\n</code></pre> <p>Fastest way to get started. This installs the latest stable release.</p>"},{"location":"installation/#option-2-install-from-github","title":"\ud83d\udd39 Option 2: Install from GitHub","text":"<p>Clone the repository and install the package locally:</p> <pre><code>git clone https://github.com/spqb/adabmDCApy.git\ncd adabmDCApy\npip install .\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCApy</p> <p>Info</p> <p>This version of the code assumes the user to be provided with a GPU. If this is not the case, we provide a Colab notebook that can be used with GPU hardware acceleration provided by Google.</p>"},{"location":"installation/#julia-multi-core-cpu","title":"\ud83d\udfe3 Julia (Multi-core CPU)","text":"<p>Make sure you\u2019ve installed Julia. Then choose one of the following:</p>"},{"location":"installation/#option-1-automatic-setup-via-shell","title":"\ud83d\udd39 Option 1: Automatic Setup via Shell","text":"<pre><code># Download main scripts\nwget -O adabmDCA.sh https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/adabmDCA.sh\nwget -O execute.jl https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/execute.jl\nchmod +x adabmDCA.sh\n\n# Install dependencies and the package\njulia --eval 'using Pkg; Pkg.add(\"ArgParse\"); Pkg.add(PackageSpec(url=\"https://github.com/spqb/adabmDCA.jl\"))'\n</code></pre>"},{"location":"installation/#option-2-manual-setup-via-julia-repl","title":"\ud83d\udd39 Option 2: Manual Setup via Julia REPL","text":"<ol> <li>Launch Julia and run:</li> </ol> <pre><code>using Pkg\nPkg.add(url=\"https://github.com/spqb/adabmDCA.jl\")\nPkg.add(\"ArgParse\")\n</code></pre> <ol> <li>Download execution scripts:</li> </ol> <pre><code>wget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/adabmDCA.sh\nwget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/execute.jl\nchmod +x adabmDCA.sh\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCA.jl</p>"},{"location":"installation/#c-single-core-cpu","title":"\ud83d\udfe6 C++ (Single-core CPU)","text":"<p>A minimal setup with no external dependencies beyond <code>make</code>.</p>"},{"location":"installation/#installation-steps","title":"\ud83d\udd39 Installation Steps","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/spqb/adabmDCAc.git\ncd adabmDCAc/src\nmake\n</code></pre> <ol> <li>Return to the root folder and make the main script executable:</li> </ol> <pre><code>chmod +x adabmDCA.sh\n</code></pre> <ol> <li>Verify installation and available options:</li> </ol> <pre><code>./adabmDCA --help\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCAc</p> <p>Tip</p> <p>All implementations share a consistent command-line interface. You can switch between them based on your hardware and performance needs without learning new syntax.</p>"},{"location":"preprocessing/","title":"Input data and preprocessing","text":""},{"location":"preprocessing/#input-data-preprocessing","title":"Input Data &amp; Preprocessing","text":""},{"location":"preprocessing/#input-format","title":"\ud83d\udce5 Input Format","text":"<p><code>adabmDCA 2.0</code> takes as input a multiple sequence alignment (MSA) in FASTA format, typically of aligned protein or RNA/DNA sequences (see Fig. 1).</p> <p>The tool supports three built-in alphabets and also allows custom alphabets, as long as they match the MSA content.</p> Type Alphabet Symbols protein <code>-, A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y</code> RNA <code>-, A, C, G, U</code> DNA <code>-, A, C, G, T</code> <p>\ud83d\udca1 Line breaks within sequences are supported.</p> <p>Figure 1: Example of a fasta file containg the MSA. </p>"},{"location":"preprocessing/#preprocessing-steps","title":"\ud83d\udd27 Preprocessing Steps","text":"<p>The following steps are applied to every input MSA:</p> <ol> <li>Remove sequences with invalid symbols.</li> <li>Remove duplicate sequences.</li> <li>Reweight sequences to correct for phylogenetic and sampling bias (optional).</li> <li>Compute empirical statistics using a pseudocount.</li> </ol>"},{"location":"preprocessing/#sequence-reweighting","title":"\u2696\ufe0f Sequence Reweighting","text":"<p>To downweight overrepresented or phylogenetically related sequences, <code>adabmDCA</code> uses a clustering threshold (default: 80% identity). The weight for sequence \\(\\mathbf{a}^{(m)}\\) is:</p> \\[ w^{(m)} = \\frac{1}{N^{(m)}} \\] <p>where \\(N^{(m)}\\) is the number of sequences that have sequence identity with \\(\\mathbf{a}^{(m)}\\) above the clustering threshold.</p> <ul> <li>Set the threshold with <code>--clustering_seqid &lt;value&gt;</code></li> <li>Disable with <code>--no_reweighting</code></li> </ul>"},{"location":"preprocessing/#pseudocount-regularization","title":"\ud83e\uddee Pseudocount Regularization","text":"<p>A small pseudocount \\(\\alpha\\) is added to frequency estimates to prevent issues with rare or unobserved symbols:</p> <ul> <li> <p>One-site frequency:  \\(f_i(a) = (1 - \\alpha) f^{\\mathrm{data}}_i(a) + \\frac{\\alpha}{q}\\)</p> </li> <li> <p>Two-site frequency: \\(f_{ij}(a, b) = (1 - \\alpha) f^{\\mathrm{data}}_{ij}(a, b) + \\frac{\\alpha}{q^2}\\)</p> </li> </ul> <p>If not set via <code>--pseudocount</code>, the default is: $$ \\alpha = \\frac{1}{M_{\\text{eff}}}, \\quad \\text{with} \\quad M_{\\text{eff}} = \\sum_{m=1}^M w^{(m)} $$ being the effective number of sequences.</p>"},{"location":"quicklist/","title":"Quicklist","text":""},{"location":"quicklist/#list-of-the-main-routines-with-standard-arguments","title":"\u26a1 List of the main routines with standard arguments","text":"<ul> <li>\ud83e\udde0 Train a <code>bmDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Resume training of a <code>bmDCA</code> model:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83c\udf31 Train an <code>eaDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --nsweeps 5\n</code></pre> <ul> <li>\ud83d\udd04 Resume training of an eaDCA model:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\u2702\ufe0f Decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83d\udd00 Train and decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt;\n</code></pre> <ul> <li>\ud83e\uddec Generate sequences from a trained model:</li> </ul> <pre><code>adabmDCA sample -p &lt;file_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li>\ud83d\udcc9 Score a sequence set:</li> </ul> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83e\uddea Generate a single mutant library from a wild type:</li> </ul> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd17 Compute contact scores via Frobenius norm:</li> </ul> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Reintegrate DCA model from experiments:</li> </ul> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <ul> <li>\ud83e\udde0 Train/test split for homologous sequences:</li> </ul> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre>"},{"location":"script_arguments/","title":"Script Arguments","text":"<p>In this section we list all the possible command-line arguments for the main routines of <code>adabmDCA 2.0</code>.</p>"},{"location":"script_arguments/#train-a-dca-model","title":"Train a DCA model","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the dataset to be used for training the model. <code>-o, --output</code> DCA_model Path to the folder where to save the model. <code>-m, --model</code> bmDCA Type of model to be trained. Possible options are <code>bmDCA</code>, <code>eaDCA</code>, and <code>edDCA</code>. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>-p, --path_params</code> None Path to the file containing the model's parameters. Required for restoring the training. <code>-c, --path_chains</code> None Path to the FASTA file containing the model's chains. Required for restoring the training. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--lr</code> 0.05 Learning rate. <code>--nsweeps</code> 10 Number of sweeps for each gradient estimation. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--nchains</code> 10000 Number of Markov chains to run in parallel. <code>--target</code> 0.95 Pearson correlation coefficient on the two-sites statistics to be reached. <code>--nepochs</code> 50000 Maximum number of epochs allowed. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--seed</code> 0 Random seed. <code>--nthreads</code>\u00b9 1 Number of threads used in the Julia multithreaded version. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#eadca-options","title":"eaDCA options","text":"Command Default value Description <code>--gsteps</code> 10 Number of gradient updates to be performed on a given graph. <code>--factivate</code> 0.001 Fraction of inactive couplings to try to activate at each graph update."},{"location":"script_arguments/#eddca-options","title":"edDCA options","text":"Command Default value Description <code>--gsteps</code> 10 The number of gradient updates applied at each step of the graph convergence process. <code>--density</code> 0.02 Target density to be reached. <code>--drate</code> 0.01 Fraction of remaining couplings to be pruned at each decimation step."},{"location":"script_arguments/#sampling-from-a-dca-model","title":"Sampling from a DCA model","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model to sample from. <code>-d, --data</code> N/A Filename of the dataset MSA. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--ngen</code> None Number of samples to generate. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>--nmeasure</code> 10000 Number of data sequences to use for computing the mixing time. The value min(<code>nmeasure</code>, len(data)) is taken. <code>--nmix</code> 2 Number of mixing times used to generate 'ngen' sequences starting from random. <code>--max_nsweeps</code> 10000 Maximum number of sweeps allowed. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--beta</code> 1.0 Inverse temperature to be used for the sampling. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#computing-dca-energies-of-a-msa","title":"Computing DCA energies of a MSA","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#generate-a-deep-mutational-scan-dms-from-a-wild-type","title":"Generate a Deep Mutational Scan (DMS) from a wild type","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA containing the wild type. If multiple sequences are present, the first one is used. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#compute-the-frobenius-contact-matrix","title":"Compute the Frobenius contact matrix","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>-l, --label</code> None If provided, adds a label to the output files inside the output folder. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version. <p>\u00b9 Used in specific versions of the software.</p>"},{"location":"training/","title":"Training DCA models \ud83d\ude80","text":"<p>All versions of adabmDCA \u2014 Python, Julia, and C++ \u2014 expose the same command-line interface through the <code>adabmDCA</code> command.</p> <p>To see the complete list of training options:</p> <pre><code>$ adabmDCA train -h\n</code></pre> <p>The standard command to start training a DCA model is:</p> <pre><code>$ adabmDCA train -m &lt;model&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -l &lt;label&gt;\n</code></pre>"},{"location":"training/#arguments","title":"Arguments \ud83e\udde9","text":"<ul> <li><code>&lt;model&gt;</code> \u2208 <code>{bmDCA, eaDCA, edDCA}</code>   Selects the training routine.   By default, the fully connected <code>bmDCA</code> algorithm is used. <code>edDCA</code> can follow two different routines: either it decimates a pre-trained <code>bmDCA</code> model, or it first trains a <code>bmDCA</code> model and then decimates it.</li> <li><code>&lt;fasta_file&gt;</code> \u2013 Path to the FASTA file containing the training MSA.</li> <li><code>&lt;output_folder&gt;</code> \u2013 Folder where results will be stored (created if missing).</li> <li><code>&lt;label&gt;</code> \u2013 Optional tag for output files.</li> </ul>"},{"location":"training/#training-behavior","title":"Training Behavior \u2699\ufe0f","text":"<p>Training stops when the Pearson correlation between model and empirical connected correlations reaches the target value (default: <code>0.95</code>).</p> <ul> <li>Early training is fast (e.g., Pearson \u2248 0.9 after ~100 iterations).  </li> <li>Approaching higher values takes significantly longer (power\u2011law decay).</li> </ul> <p>For a quick coarse model, set:</p> <pre><code>--target 0.9\n</code></pre>"},{"location":"training/#output-files","title":"Output Files \ud83d\udcc1","text":"<p>During training, adabmDCA maintains three output files:</p> <ul> <li><code>&lt;label&gt;_params.dat</code> \u2013 Non\u2011zero model parameters  </li> <li>Lines starting with <code>J</code> \u2192 couplings  </li> <li> <p>Lines with <code>h</code> \u2192 biases</p> </li> <li> <p><code>&lt;label&gt;_chains.fasta</code> \u2013 State of the Markov chains</p> </li> <li> <p><code>&lt;label&gt;_adabmDCA.log</code> \u2013 Log file updated throughout training</p> </li> </ul> <p>Update intervals: - <code>bmDCA</code>: every 50 updates - <code>eaDCA</code>, <code>edDCA</code>: every 10 updates  </p>"},{"location":"training/#restoring-interrupted-training","title":"Restoring Interrupted Training \ud83d\udd04","text":"<p>Resume training using:</p> <pre><code>$ adabmDCA train [...] -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre>"},{"location":"training/#importance-weights","title":"Importance Weights \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f","text":"<p>Provide custom weights with:</p> <pre><code>--weights &lt;path&gt;\n</code></pre> <p>Otherwise, weights are computed automatically and stored as:</p> <pre><code>&lt;label&gt;_weights.dat\n</code></pre> <p>Options:</p> <ul> <li><code>--clustering_seqid &lt;value&gt;</code> \u2013 default: 0.8  </li> <li><code>--no_reweighting</code> \u2013 use uniform weights  </li> </ul>"},{"location":"training/#choosing-the-alphabet","title":"Choosing the Alphabet \ud83d\udd20","text":"<p>Default alphabet: protein.</p> <p>Specify alternatives:</p> <ul> <li>RNA \u2192 <code>--alphabet rna</code></li> <li>DNA \u2192 <code>--alphabet dna</code></li> <li>Custom \u2192 <code>--alphabet ABCD-</code></li> </ul>"},{"location":"training/#eadca","title":"eaDCA \ud83c\udf31","text":"<p>Enable with:</p> <pre><code>--model eaDCA\n</code></pre> <p>Key hyperparameters:</p> <ul> <li><code>--factivate</code> \u2013 fraction of inactive couplings activated (default: 0.001)  </li> <li><code>--gsteps</code> \u2013 parameter updates per graph update (default: 10)</li> </ul> <p>Recommended: reduce sweeps to 5.</p>"},{"location":"training/#eddca-decimated-dca","title":"edDCA \u2702\ufe0f (Decimated DCA)","text":"<p>Run decimation:</p> <pre><code>$ adabmDCA train -m edDCA -d &lt;fasta_file&gt; -p &lt;params&gt; -c &lt;chains&gt;\n</code></pre> <p>Two workflows:</p> <ol> <li>Use pre\u2011trained bmDCA (<code>params</code> + <code>chains</code>)</li> <li>Train bmDCA automatically, then decimate</li> </ol> <p>Key hyperparameters:</p> <ul> <li><code>--gsteps</code> \u2013 default: 10  </li> <li><code>--drate</code> \u2013 pruning fraction (default: 0.01)  </li> <li><code>--density</code> \u2013 target graph density (default: 0.02)  </li> <li><code>--target</code> \u2013 Pearson threshold (default: 0.95)</li> </ul>"},{"location":"training/#choosing-hyperparameters","title":"Choosing Hyperparameters \ud83c\udf9a\ufe0f","text":"<p>Defaults work well for clean and moderately diverse MSAs. For more difficult datasets, consider tuning:</p>"},{"location":"training/#learning-rate","title":"Learning Rate","text":"<ul> <li>Default: 0.01 </li> <li>If chains mix poorly, try: <code>--lr 0.005</code></li> </ul>"},{"location":"training/#number-of-markov-chains","title":"Number of Markov Chains","text":"<ul> <li>Default: 10,000 </li> <li>Using fewer chains reduces the memory required to train the model, but it may also lead to a longer algorithm convergence time.  </li> <li>Change with: <code>--nchains &lt;value&gt;</code></li> </ul>"},{"location":"training/#number-of-monte-carlo-steps","title":"Number of Monte Carlo Steps","text":"<ul> <li>Controlled by <code>--nsweeps</code> </li> <li>Default: 10 </li> <li>Recommended range: 10\u201350. Higher values drastically increase the training time and, empirically, do not help much the model convergence.</li> </ul>"},{"location":"training/#regularization-pseudocount","title":"Regularization (Pseudocount)","text":"<p>Controlled by <code>--pseudocount</code>.</p> <p>Default:</p> <pre><code>\u03b1 = 1 / M_eff\n</code></pre> <p>Increasing \u03b1 (e.g. \u03b1 = 0.001 or 0.01) may help when the training struggle converging or the mixing time of the model is very high, but it also makes the model less expressive.</p>"},{"location":"api/","title":"Overview","text":""},{"location":"api/#api-overview","title":"API Overview","text":""},{"location":"api/#modules","title":"Modules","text":"<ul> <li><code>adabmDCA.checkpoint</code></li> <li><code>adabmDCA.cobalt</code></li> <li><code>adabmDCA.dataset</code></li> <li><code>adabmDCA.dca</code></li> <li><code>adabmDCA.fasta</code></li> <li><code>adabmDCA.functional</code></li> <li><code>adabmDCA.graph</code></li> <li><code>adabmDCA.io</code></li> <li><code>adabmDCA.plot</code></li> <li><code>adabmDCA.resampling</code></li> <li><code>adabmDCA.sampling</code></li> <li><code>adabmDCA.statmech</code></li> <li><code>adabmDCA.stats</code></li> <li><code>adabmDCA.training</code></li> <li><code>adabmDCA.utils</code></li> </ul>"},{"location":"api/#classes","title":"Classes","text":"<ul> <li><code>checkpoint.Checkpoint</code>: Helper class to save the model's parameters and chains at regular intervals during training and to log the</li> <li><code>dataset.DatasetDCA</code>: Dataset class for handling multi-sequence alignments data.</li> </ul>"},{"location":"api/#functions","title":"Functions","text":"<ul> <li><code>cobalt.prune_redundant_sequences</code>: Prunes sequences from X such that no sequence has more than 'seqid_th' fraction of its residues identical to any other sequence in the set.</li> <li><code>cobalt.run_cobalt</code>: Runs the Cobalt algorithm to split the input MSA into training and test sets.</li> <li><code>cobalt.split_train_test</code>: Splits X into two sets, T and S, such that no sequence in S has more than</li> <li><code>dca.get_contact_map</code>: Computes the contact map from the model coupling matrix.</li> <li><code>dca.get_mf_contact_map</code>: Computes the contact map using mean-field approximation from the data.</li> <li><code>dca.get_seqid</code>: Returns a tensor containing the sequence identities between two sets of one-hot encoded sequences.</li> <li><code>dca.get_seqid_stats</code>: - If s2 is provided, computes the mean and the standard deviation of the mean sequence identity between two sets of one-hot encoded sequences.</li> <li><code>dca.set_zerosum_gauge</code>: Sets the zero-sum gauge on the coupling matrix.</li> <li><code>fasta.compute_weights</code>: Computes the weight to be assigned to each sequence 's' in 'data' as 1 / n_clust, where 'n_clust' is the number of sequences</li> <li><code>fasta.decode_sequence</code>: Takes a numeric sequence or list of seqences in input an returns the corresponding string encoding.</li> <li><code>fasta.encode_sequence</code>: Encodes a sequence or a list of sequences into a numeric format.</li> <li><code>fasta.get_tokens</code>: Converts a known alphabet into the corresponding tokens, otherwise returns the custom alphabet.</li> <li><code>fasta.import_from_fasta</code>: Import sequences from a fasta or compressed fasta (.fas.gz) file. The following operations are performed:</li> <li><code>fasta.validate_alphabet</code>: Check if the chosen alphabet is compatible with the input sequences.</li> <li><code>fasta.write_fasta</code>: Generate a fasta file with the input sequences.</li> <li><code>functional.one_hot</code>: A fast one-hot encoding function faster than the PyTorch one working with torch.int32 and returning a float Tensor.</li> <li><code>graph.decimate_graph</code>: Performs one decimation step and updates the parameters and mask.</li> <li><code>graph.update_mask_activation</code>: Updates the mask by removing the nactivate couplings with the smallest Dkl.</li> <li><code>graph.update_mask_decimation</code>: Updates the mask by removing the n_remove couplings with the smallest Dkl.</li> <li><code>io.load_chains</code>: Loads the sequences from a fasta file and returns the one-hot encoded version.</li> <li><code>io.load_params</code>: Import the parameters of the model from a text file.</li> <li><code>io.load_params_old</code>: Import the parameters of the model from a file.</li> <li><code>io.load_params_oldformat</code>: Import the parameters of the model from a file. Assumes the old DCA format.</li> <li><code>io.save_chains</code>: Saves the chains in a fasta file.</li> <li><code>io.save_params</code>: Saves the parameters of the model in a file.</li> <li><code>io.save_params_oldformat</code>: Saves the parameters of the model in a file. Assumes the old DCA format.</li> <li><code>plot.plot_PCA</code>: Makes the scatter plot of the components (pc1, pc2) of the input data and shows the histograms of the components.</li> <li><code>plot.plot_autocorrelation</code>: Plots the time-autocorrelation curve of the sequence identity and the generated and data sequence identities.</li> <li><code>plot.plot_contact_map</code>: Plots the contact map.</li> <li><code>plot.plot_pearson_sampling</code>: Plots the Pearson correlation coefficient over sampling time.</li> <li><code>plot.plot_scatter_correlations</code>: Plots the scatter plot of the data and generated Cij and Cijk values.</li> <li><code>resampling.compute_mixing_time</code>: Computes the mixing time using the t and t/2 method. The sampling will halt when the mixing time is reached or</li> <li><code>sampling.get_sampler</code>: Returns the sampling function corresponding to the chosen method.</li> <li><code>sampling.gibbs_sampling</code>: Gibbs sampling. Attempts L * nsweeps mutations to each sequence in 'chains'.</li> <li><code>sampling.gibbs_step_independent_sites</code>: Performs a single mutation using the Gibbs sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.gibbs_step_uniform_sites</code>: Performs a single mutation using the Gibbs sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>sampling.metropolis_sampling</code>: Metropolis sampling. Attempts L * nsweeps mutations to each sequence in 'chains'.</li> <li><code>sampling.metropolis_step_independent_sites</code>: Performs a single mutation using the Metropolis sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.metropolis_step_uniform_sites</code>: Performs a single mutation using the Metropolis sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>sampling.sampling_profile</code>: Samples from the profile model defined by the local biases only.</li> <li><code>statmech.compute_energy</code>: Compute the DCA energy for a batch of sequences.</li> <li><code>statmech.compute_entropy</code>: Compute the entropy of the DCA model.</li> <li><code>statmech.compute_logZ_exact</code>: Compute the log-partition function of the model.</li> <li><code>statmech.compute_log_likelihood</code>: Compute the log-likelihood of the model.</li> <li><code>statmech.enumerate_states</code>: Enumerate all possible states of a system of L sites and q states.</li> <li><code>statmech.iterate_tap</code>: Iterates the TAP equations until convergence.</li> <li><code>stats.extract_Cij_from_freq</code>: Extracts the lower triangular part of the covariance matrices of the natural data and generated data starting from the frequencies.</li> <li><code>stats.extract_Cij_from_seqs</code>: Extracts the lower triangular part of the covariance matrices of the natural data and generated data starting from the sequences.</li> <li><code>stats.generate_unique_triplets</code>: Generates a set of unique triplets of positions. Used to compute the 3-points statistics.</li> <li><code>stats.get_correlation_two_points</code>: Computes the Pearson coefficient and the slope between the two-point frequencies of data and chains.</li> <li><code>stats.get_covariance_matrix</code>: Computes the weighted covariance matrix of the input multi sequence alignment.</li> <li><code>stats.get_freq_single_point</code>: Computes the single point frequencies of the input MSA.</li> <li><code>stats.get_freq_three_points</code>: Computes the 3-body connected correlation statistics of the input MSAs.</li> <li><code>stats.get_freq_two_points</code>: Computes the 2-points statistics of the input MSA.</li> <li><code>training.train_eaDCA</code>: Fits an eaDCA model on the training data and saves the results in a file.</li> <li><code>training.train_edDCA</code>: Fits an edDCA model on the training data and saves the results in a file.</li> <li><code>training.train_graph</code>: Trains the model on a given graph until the target Pearson correlation is reached or the maximum number of epochs is exceeded.</li> <li><code>training.update_params</code>: Updates the parameters of the model.</li> <li><code>utils.get_device</code>: Returns the device where to store the tensors.</li> <li><code>utils.get_dtype</code>: Returns the data type of the tensors.</li> <li><code>utils.get_mask_save</code>: Returns the mask to save the upper-triangular part of the coupling matrix.</li> <li><code>utils.init_chains</code>: Initialize the Markov chains of the DCA model. If 'fi' is provided, the chains are sampled from the</li> <li><code>utils.init_parameters</code>: Initialize the parameters of the DCA model. The bias terms are initialized</li> <li><code>utils.resample_sequences</code>: Extracts nextract sequences from data with replacement according to the weights.</li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.checkpoint/","title":"adabmDCA.checkpoint","text":""},{"location":"api/adabmDCA.checkpoint/#module-adabmdcacheckpoint","title":"module <code>adabmDCA.checkpoint</code>","text":""},{"location":"api/adabmDCA.checkpoint/#class-checkpoint","title":"class <code>Checkpoint</code>","text":"<p>Helper class to save the model's parameters and chains at regular intervals during training and to log the progress of the training. </p> <p></p>"},{"location":"api/adabmDCA.checkpoint/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(\n    file_paths: dict,\n    tokens: str,\n    args: dict,\n    params: Optional[Dict[str, Tensor]] = None,\n    chains: Optional[Tensor] = None,\n    use_wandb: bool = False\n)\n</code></pre> <p>Initializes the Checkpoint class. </p> <p>Args:</p> <ul> <li><code>file_paths</code> (dict):  Dictionary containing the paths of the files to be saved. </li> <li><code>tokens</code> (str):  Alphabet to be used for encoding the sequences. </li> <li><code>args</code> (dict):  Dictionary containing the arguments of the training. </li> <li><code>params</code> (Optional[Dict[str, torch.Tensor]], optional):  Parameters of the model. Defaults to None. </li> <li><code>chains</code> (Optional[torch.Tensor], optional):  Chains. Defaults to None. </li> <li><code>use_wandb</code> (bool, optional):  Whether to use Weights &amp; Biases for logging. Defaults to False. </li> </ul> <p></p>"},{"location":"api/adabmDCA.checkpoint/#method-check","title":"method <code>check</code>","text":"<pre><code>check(updates: int) \u2192 bool\n</code></pre> <p>Checks if a checkpoint has been reached. </p> <p>Args:</p> <ul> <li><code>updates</code> (int):  Number of gradient updates performed. </li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>:  Whether a checkpoint has been reached. </li> </ul> <p></p>"},{"location":"api/adabmDCA.checkpoint/#method-log","title":"method <code>log</code>","text":"<pre><code>log(record: Dict[str, Any]) \u2192 None\n</code></pre> <p>Adds a key-value pair to the log dictionary </p> <p>Args:</p> <ul> <li><code>record</code> (Dict[str, Any]):  Key-value pairs to be added to the log dictionary. </li> </ul> <p></p>"},{"location":"api/adabmDCA.checkpoint/#method-save","title":"method <code>save</code>","text":"<pre><code>save(\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    chains: Tensor,\n    log_weights: Tensor\n) \u2192 None\n</code></pre> <p>Saves the chains and the parameters of the model. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the model's coupling matrix representing the interaction graph </li> <li><code>chains</code> (torch.Tensor):  Chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log of the chain weights. Used for AIS. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.cobalt/","title":"Train/test split","text":""},{"location":"api/adabmDCA.cobalt/#module-adabmdcacobalt","title":"module <code>adabmDCA.cobalt</code>","text":""},{"location":"api/adabmDCA.cobalt/#function-split_train_test","title":"function <code>split_train_test</code>","text":"<pre><code>split_train_test(\n    headers: ndarray,\n    X: Tensor,\n    seqid_th: float,\n    rnd_gen: Optional[Generator] = None\n) \u2192 Tuple[ndarray, Tensor, ndarray, Tensor]\n</code></pre> <p>Splits X into two sets, T and S, such that no sequence in S has more than 'seqid_th' fraction of its residues identical to any sequence in T. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA, shape (batch_size, L). </li> <li><code>seqid_th</code> (float):  Threshold sequence identity. </li> <li><code>rnd_gen</code> (Optional[torch.Generator], optional):  Random number generator. Defaults to None. </li> </ul> <p>Returns:  Training and test sets as:  (np.ndarray) Training headers,  (torch.Tensor) Training sequences,  (np.ndarray) Test headers,  (torch.Tensor) Test sequences. </p> <p></p>"},{"location":"api/adabmDCA.cobalt/#function-prune_redundant_sequences","title":"function <code>prune_redundant_sequences</code>","text":"<pre><code>prune_redundant_sequences(\n    headers: ndarray,\n    X: Tensor,\n    seqid_th: float,\n    rnd_gen: Optional[Generator] = None\n) \u2192 Tuple[ndarray, Tensor]\n</code></pre> <p>Prunes sequences from X such that no sequence has more than 'seqid_th' fraction of its residues identical to any other sequence in the set. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA. </li> <li><code>seqid_th</code> (float):  Threshold sequence identity. </li> <li><code>rnd_gen</code> (Optional[torch.Generator], optional):  Random generator. Defaults to None. </li> </ul> <p>Returns:  Tuple[np.ndarray, torch.Tensor]:  (np.ndarray) Headers of pruned sequences  (torch.Tensor) Pruned sequences. </p> <p></p>"},{"location":"api/adabmDCA.cobalt/#function-run_cobalt","title":"function <code>run_cobalt</code>","text":"<pre><code>run_cobalt(\n    headers: ndarray,\n    X: Tensor,\n    t1: float,\n    t2: float,\n    t3: float,\n    max_train: Optional[int] = None,\n    max_test: Optional[int] = None,\n    rnd_gen: Optional[Generator] = None\n) \u2192 Tuple[ndarray, Tensor, ndarray, Tensor]\n</code></pre> <p>Runs the Cobalt algorithm to split the input MSA into training and test sets. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA. </li> <li><code>t1</code> (float):  No sequence in S has more than this fraction of its residues identical to any sequence in T. </li> <li><code>t2</code> (float):  No pair of test sequences has more than this value fractional identity. </li> <li><code>t3</code> (float):  No pair of training sequences has more than this value fractional identity. </li> <li><code>max_train</code> (Optional[int], optional):  Maximum number of sequences in the training set. Defaults to None. </li> <li><code>max_test</code> (Optional[int], optional):  Maximum number of sequences in the test set. Defaults to None. </li> <li><code>rnd_gen</code> (Optional[torch.Generator], optional):  Random number generator. Defaults to None. </li> </ul> <p>Returns:  Training and test sets as:  (np.ndarray) Training headers,  (torch.Tensor) Training sequences,  (np.ndarray) Test headers,  (torch.Tensor) Test sequences. </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.dataset/","title":"Dataset class","text":""},{"location":"api/adabmDCA.dataset/#module-adabmdcadataset","title":"module <code>adabmDCA.dataset</code>","text":""},{"location":"api/adabmDCA.dataset/#class-datasetdca","title":"class <code>DatasetDCA</code>","text":"<p>Dataset class for handling multi-sequence alignments data. </p> <p></p>"},{"location":"api/adabmDCA.dataset/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(\n    path_data: str,\n    path_weights: Optional[str] = None,\n    alphabet: str = 'protein',\n    clustering_th: float = 0.8,\n    no_reweighting: bool = False,\n    remove_duplicates: bool = False,\n    filter_sequences: bool = False,\n    message: bool = True,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32\n)\n</code></pre> <p>Initialize the dataset. </p> <p>Args:</p> <ul> <li><code>path_data</code> (str):  Path to multi sequence alignment in fasta format. </li> <li><code>path_weights</code> (Optional[str], optional):  Path to the file containing the importance weights of the sequences. If None, the weights are computed automatically. </li> <li><code>alphabet</code> (str, optional):  Selects the type of encoding of the sequences. Default choices are (\"protein\", \"rna\", \"dna\"). Defaults to \"protein\". </li> <li><code>clustering_th</code> (float, optional):  Sequence identity threshold for clustering. Defaults to 0.8. </li> <li><code>no_reweighting</code> (bool, optional):  If True, the weights are not computed. Defaults to False. </li> <li><code>remove_duplicates</code> (bool, optional):  If True, removes duplicate sequences from the dataset. Defaults to False. </li> <li><code>filter_sequences</code> (bool, optional):  If True, removes sequences containing tokens not in the alphabet. Defaults to False. </li> <li><code>message</code> (bool, optional):  Print the import message. Defaults to True. </li> <li><code>device</code> (torch.device, optional):  Device to be used. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the dataset. Defaults to torch.float32. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dataset/#method-get_effective_size","title":"method <code>get_effective_size</code>","text":"<pre><code>get_effective_size() \u2192 int\n</code></pre> <p>Returns the effective size (Meff) of the dataset. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Effective size of the dataset. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dataset/#method-get_num_residues","title":"method <code>get_num_residues</code>","text":"<pre><code>get_num_residues() \u2192 int\n</code></pre> <p>Returns the number of residues (L) in the multi-sequence alignment. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Length of the MSA. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dataset/#method-get_num_states","title":"method <code>get_num_states</code>","text":"<pre><code>get_num_states() \u2192 int\n</code></pre> <p>Returns the number of states (q) in the alphabet. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Number of states. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dataset/#method-shuffle","title":"method <code>shuffle</code>","text":"<pre><code>shuffle() \u2192 None\n</code></pre> <p>Shuffles the dataset.  </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.dca/","title":"DCA utils","text":""},{"location":"api/adabmDCA.dca/#module-adabmdcadca","title":"module <code>adabmDCA.dca</code>","text":""},{"location":"api/adabmDCA.dca/#function-get_seqid","title":"function <code>get_seqid</code>","text":"<pre><code>get_seqid(s1: Tensor, s2: Optional[Tensor] = None) \u2192 Tensor\n</code></pre> <p>Returns a tensor containing the sequence identities between two sets of one-hot encoded sequences.  - If s2 is provided, computes the sequence identity between the corresponding sequences in s1 and s2.  - If s2 is a single sequence (L, q), it computes the sequence identities between the dataset s1 and s2.  - If s2 is none, computes the sequence identity between s1 and a permutation of s1. </p> <p>Args:</p> <ul> <li><code>s1</code> (torch.Tensor):  One-hot encoded sequence dataset 1 of shape (batch_size, L, q) or (L, q). </li> <li><code>s2</code> (Optional[torch.Tensor]):  One-hot encoded sequence dataset 2 of shape (batch_size, L, q) or (L, q). Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Tensor of sequence identities. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dca/#function-get_seqid_stats","title":"function <code>get_seqid_stats</code>","text":"<pre><code>get_seqid_stats(s1: Tensor, s2: Optional[Tensor] = None) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <ul> <li>If s2 is provided, computes the mean and the standard deviation of the mean sequence identity between two sets of one-hot encoded sequences. </li> <li>If s2 is a single sequence (L, q), it computes the mean and the standard deviation of the mean sequence identity between the dataset s1 and s2. </li> <li>If s2 is none, computes the mean and the standard deviation of the mean of the sequence identity between s1 and a permutation of s1. </li> </ul> <p>Args:</p> <ul> <li><code>s1</code> (torch.Tensor):  One-hot encoded sequence dataset 1 of shape (batch_size, L, q) or (L, q). </li> <li><code>s2</code> (Optional[torch.Tensor]):  One-hot encoded sequence dataset 2 of shape (batch_size, L, q) or (L, q). Defaults to None. </li> </ul> <p>Returns:  Tuple[torch.Tensor, torch.Tensor]:  (torch.Tensor) Mean sequence identity  (torch.Tensor) Standard deviation of the mean sequence identity. </p> <p></p>"},{"location":"api/adabmDCA.dca/#function-set_zerosum_gauge","title":"function <code>set_zerosum_gauge</code>","text":"<pre><code>set_zerosum_gauge(params: Dict[str, Tensor]) \u2192 Dict[str, Tensor]\n</code></pre> <p>Sets the zero-sum gauge on the coupling matrix. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  New dictionary with modified coupling matrix. </li> <li><code>\"bias\"</code>:  torch.Tensor of shape (L, q) </li> <li><code>\"coupling_matrix\"</code>:  torch.Tensor of shape (L, q, L, q) </li> </ul> <p></p>"},{"location":"api/adabmDCA.dca/#function-get_contact_map","title":"function <code>get_contact_map</code>","text":"<pre><code>get_contact_map(params: Dict[str, Tensor], tokens: str) \u2192 ndarray\n</code></pre> <p>Computes the contact map from the model coupling matrix. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Model parameters. Should contain:          - \"coupling_matrix\": torch.Tensor of shape (L, q, L, q)          - \"bias\": torch.Tensor of shape (L, q) </li> <li><code>tokens</code> (str):  Alphabet to be used. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Contact map. </li> </ul> <p></p>"},{"location":"api/adabmDCA.dca/#function-get_mf_contact_map","title":"function <code>get_mf_contact_map</code>","text":"<pre><code>get_mf_contact_map(\n    data: Tensor,\n    tokens: str,\n    weights: Optional[Tensor] = None\n) \u2192 ndarray\n</code></pre> <p>Computes the contact map using mean-field approximation from the data. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Input one-hot data tensor. </li> <li><code>tokens</code> (str):  Alphabet to be used. </li> <li><code>weights</code> (Optional[torch.Tensor]):  Weights for the data points. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Contact map. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.fasta/","title":"Fasta utils","text":""},{"location":"api/adabmDCA.fasta/#module-adabmdcafasta","title":"module <code>adabmDCA.fasta</code>","text":""},{"location":"api/adabmDCA.fasta/#global-variables","title":"Global Variables","text":"<ul> <li>TOKENS_PROTEIN</li> <li>TOKENS_RNA</li> <li>TOKENS_DNA</li> </ul>"},{"location":"api/adabmDCA.fasta/#function-get_tokens","title":"function <code>get_tokens</code>","text":"<pre><code>get_tokens(alphabet: str) \u2192 str\n</code></pre> <p>Converts a known alphabet into the corresponding tokens, otherwise returns the custom alphabet. </p> <p>Args:</p> <ul> <li><code>alphabet</code> (str):  Alphabet to be used for the encoding. It can be either \"protein\", \"rna\", \"dna\" or a custom string of tokens. </li> </ul> <p>Returns:</p> <ul> <li><code>str</code>:  Tokens of the alphabet. </li> </ul> <p></p>"},{"location":"api/adabmDCA.fasta/#function-encode_sequence","title":"function <code>encode_sequence</code>","text":"<pre><code>encode_sequence(sequence: Union[str, Iterable[str]], tokens: str) \u2192 ndarray\n</code></pre> <p>Encodes a sequence or a list of sequences into a numeric format. </p> <p>Args:</p> <ul> <li><code>sequence</code> (Union[str, Iterable[str]]):  Input sequence or iterable of sequences of size (batch_size,). </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Array of shape (L,) or (batch_size, L) with the encoded sequence or sequences. </li> </ul> <p></p>"},{"location":"api/adabmDCA.fasta/#function-decode_sequence","title":"function <code>decode_sequence</code>","text":"<pre><code>decode_sequence(\n    sequence: Union[ndarray, Tensor, list],\n    tokens: str\n) \u2192 Union[str, ndarray]\n</code></pre> <p>Takes a numeric sequence or list of seqences in input an returns the corresponding string encoding. </p> <p>Args:</p> <ul> <li><code>sequence</code> (Union[np.ndarray, torch.Tensor, list]):  Input sequences. Can be of shape          - (L,): single sequence in encoded format          - (batch_size, L): multiple sequences in encoded format          - (batch_size, L, q) multiple one-hot encoded sequences </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Returns:</p> <ul> <li><code>Union[str, np.ndarray]</code>:  string or array of strings with the decoded input. </li> </ul> <p></p>"},{"location":"api/adabmDCA.fasta/#function-import_from_fasta","title":"function <code>import_from_fasta</code>","text":"<pre><code>import_from_fasta(\n    fasta_name: str,\n    tokens: Optional[str] = None,\n    filter_sequences: bool = False,\n    remove_duplicates: bool = False,\n    return_mask: bool = False\n)\n</code></pre> <p>Import sequences from a fasta or compressed fasta (.fas.gz) file. The following operations are performed:  - If 'tokens' is provided, encodes the sequences in numeric format.  - If 'filter_sequences' is True, removes the sequences whose tokens are not present in the alphabet.  - If 'remove_duplicates' is True, removes the duplicated sequences.  - If 'return_mask' is True, returns also the mask selecting the retained sequences from the original ones. </p> <p>Args:</p> <ul> <li><code>fasta_name</code> (str | Path):  Path to the fasta or compressed fasta (.fas.gz) file. </li> <li><code>tokens</code> (str | None, optional):  Alphabet to be used for the encoding. If provided, encodes the sequences in numeric format. </li> <li><code>filter_sequences</code> (bool, optional):  If True, removes the sequences whose tokens are not present in the alphabet. Defaults to False. </li> <li><code>remove_duplicates</code> (bool, optional):  If True, removes the duplicated sequences. Defaults to False. </li> <li><code>return_mask</code> (bool, optional):  If True, returns also the mask selecting the retained sequences from the original ones. Defaults to False. </li> </ul> <p>Raises:</p> <ul> <li><code>RuntimeError</code>:  The file is not in fasta format. </li> </ul> <p>Returns:  Union[Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, np.ndarray]]:      - If 'return_mask' is False: Tuple of (headers, sequences)      - If 'return_mask' is True: Tuple of (headers, sequences, mask) </p> <p></p>"},{"location":"api/adabmDCA.fasta/#function-write_fasta","title":"function <code>write_fasta</code>","text":"<pre><code>write_fasta(\n    fname: str,\n    headers: Union[Iterable[str], ndarray, Tensor],\n    sequences: Union[Iterable[str], ndarray, Tensor],\n    remove_gaps: bool = False,\n    tokens: str = 'protein'\n) \u2192 None\n</code></pre> <p>Generate a fasta file with the input sequences. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Name of the output fasta file. </li> <li><code>headers</code> (Union[Iterable[str], np.ndarray, torch.Tensor]):  Iterable with sequences' headers. </li> <li><code>sequences</code> (Union[Iterable[str], np.ndarray, torch.Tensor]):  Iterable with sequences in string, categorical or one-hot encoded format. </li> <li><code>remove_gaps</code> (bool, optional):  If True, removes the gap from the alignment. Defaults to False. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. Defaults to 'protein'. </li> </ul> <p></p>"},{"location":"api/adabmDCA.fasta/#function-compute_weights","title":"function <code>compute_weights</code>","text":"<pre><code>compute_weights(\n    data: Union[ndarray, Tensor],\n    th: float = 0.8,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32\n) \u2192 Tensor\n</code></pre> <p>Computes the weight to be assigned to each sequence 's' in 'data' as 1 / n_clust, where 'n_clust' is the number of sequences that have a sequence identity with 's' &gt;= th. </p> <p>Args:</p> <ul> <li><code>data</code> (Union[np.ndarray, torch.Tensor]):  Input dataset. Must be either a (batch_size, L) or a (batch_size, L, q) (one-hot encoded) array. </li> <li><code>th</code> (float, optional):  Sequence identity threshold for the clustering. Defaults to 0.8. </li> <li><code>device</code> (torch.device, optional):  Device. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Array with the weights of the sequences. </li> </ul> <p></p>"},{"location":"api/adabmDCA.fasta/#function-validate_alphabet","title":"function <code>validate_alphabet</code>","text":"<pre><code>validate_alphabet(sequences: ndarray, tokens: str)\n</code></pre> <p>Check if the chosen alphabet is compatible with the input sequences. </p> <p>Args:</p> <ul> <li><code>sequences</code> (np.ndarray):  Input sequences. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>:  The chosen alphabet is incompatible with the Multi-Sequence Alignment. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.functional/","title":"Special functions","text":""},{"location":"api/adabmDCA.functional/#module-adabmdcafunctional","title":"module <code>adabmDCA.functional</code>","text":""},{"location":"api/adabmDCA.functional/#function-one_hot","title":"function <code>one_hot</code>","text":"<pre><code>one_hot(x: Tensor, num_classes: int = -1, dtype: dtype = torch.float32) \u2192 Tensor\n</code></pre> <p>A fast one-hot encoding function faster than the PyTorch one working with torch.int32 and returning a float Tensor. Works for both 1D (single sequence) and 2D (batch of sequences) tensors. </p> <p>Args:</p> <ul> <li><code>x</code> (torch.Tensor):  Input tensor to be one-hot encoded. Shape (L,) or (batch_size, L). </li> <li><code>num_classes</code> (int, optional):  Number of classes. If -1, the number of classes is inferred from the input tensor. Defaults to -1. </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the output tensor. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  One-hot encoded tensor. Shape (L, num_classes) for 1D input or (batch_size, L, num_classes) for 2D input. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.graph/","title":"adabmDCA.graph","text":""},{"location":"api/adabmDCA.graph/#module-adabmdcagraph","title":"module <code>adabmDCA.graph</code>","text":""},{"location":"api/adabmDCA.graph/#function-update_mask_activation","title":"function <code>update_mask_activation</code>","text":"<pre><code>update_mask_activation(Dkl: Tensor, mask: Tensor, nactivate: int) \u2192 Tensor\n</code></pre> <p>Updates the mask by removing the nactivate couplings with the smallest Dkl. </p> <p>Args:</p> <ul> <li><code>Dkl</code> (torch.Tensor):  Kullback-Leibler divergence matrix. </li> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>nactivate</code> (int):  Number of couplings to be activated at each graph update. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated mask. </li> </ul> <p></p>"},{"location":"api/adabmDCA.graph/#function-update_mask_decimation","title":"function <code>update_mask_decimation</code>","text":"<pre><code>update_mask_decimation(mask: Tensor, Dkl: Tensor, drate: float) \u2192 Tensor\n</code></pre> <p>Updates the mask by removing the n_remove couplings with the smallest Dkl. </p> <p>Args:</p> <ul> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>Dkl</code> (torch.Tensor):  Kullback-Leibler divergence matrix. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated mask. </li> </ul> <p></p>"},{"location":"api/adabmDCA.graph/#function-decimate_graph","title":"function <code>decimate_graph</code>","text":"<pre><code>decimate_graph(\n    pij: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    drate: float\n) \u2192 Tuple[Dict[str, Tensor], Tensor]\n</code></pre> <p>Performs one decimation step and updates the parameters and mask. </p> <p>Args:</p> <ul> <li><code>pij</code> (torch.Tensor):  Two-point marginal probability distribution. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[Dict[str, torch.Tensor], torch.Tensor]</code>:  Updated parameters and mask. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.io/","title":"Import and export","text":""},{"location":"api/adabmDCA.io/#module-adabmdcaio","title":"module <code>adabmDCA.io</code>","text":""},{"location":"api/adabmDCA.io/#function-load_chains","title":"function <code>load_chains</code>","text":"<pre><code>load_chains(\n    fname: str,\n    tokens: str,\n    load_weights: bool = False,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32\n) \u2192 Tuple[Tensor, ]\n</code></pre> <p>Loads the sequences from a fasta file and returns the one-hot encoded version. If the sequences are weighted, the log-weights are also returned. If the sequences are not weighted, the log-weights are set to 0. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file containing the sequences. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>load_weights</code> (bool, optional):  If True, the log-weights are loaded and returned. Defaults to False. </li> <li><code>device</code> (torch.device, optional):  Device where to store the sequences. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the sequences. Defaults to torch.float32 </li> </ul> <p>Return:   - <code>Tuple[torch.Tensor, ...]</code>:  One-hot encoded sequences and log-weights if load_weights is True. </p> <p></p>"},{"location":"api/adabmDCA.io/#function-save_chains","title":"function <code>save_chains</code>","text":"<pre><code>save_chains(\n    fname: str,\n    chains: Union[list, ndarray, Tensor],\n    tokens: str,\n    log_weights: Optional[Tensor, ndarray] = None\n) \u2192 None\n</code></pre> <p>Saves the chains in a fasta file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the chains. </li> <li><code>chains</code> (Union[list, np.ndarray, torch.Tensor]):  Iterable with sequences in string, categorical or one-hot encoded format. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>log_weights</code> (Union[torch.Tensor, np.ndarray, None], optional):  Log-weights of the chains. Defaults to None. </li> </ul> <p></p>"},{"location":"api/adabmDCA.io/#function-load_params","title":"function <code>load_params</code>","text":"<pre><code>load_params(\n    fname: str,\n    tokens: str,\n    device: device,\n    dtype: dtype = torch.float32\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Import the parameters of the model from a text file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path of the file that stores the parameters. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with a compatible alphabet to be used. </li> <li><code>device</code> (torch.device):  Device where to store the parameters. </li> <li><code>dtype</code> (torch.dtype):  Data type of the parameters. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> </ul> <p></p>"},{"location":"api/adabmDCA.io/#function-load_params_old","title":"function <code>load_params_old</code>","text":"<pre><code>load_params_old(\n    fname: str,\n    tokens: str,\n    device: device,\n    dtype: dtype = torch.float32\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Import the parameters of the model from a file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path of the file that stores the parameters. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with a compatible alphabet to be used. </li> <li><code>device</code> (torch.device):  Device where to store the parameters. </li> <li><code>dtype</code> (torch.dtype):  Data type of the parameters. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> </ul> <p></p>"},{"location":"api/adabmDCA.io/#function-save_params","title":"function <code>save_params</code>","text":"<pre><code>save_params(\n    fname: str,\n    params: Dict[str, Tensor],\n    tokens: str,\n    mask: Optional[Tensor] = None\n) \u2192 None\n</code></pre> <p>Saves the parameters of the model in a file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the parameters. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with a compatible alphabet to be used. </li> <li><code>mask</code> (Optional[torch.Tensor]):  Tensor of shape (L, q, L, q) - Mask of the coupling matrix that determines which are the non-zero entries.  If None, the lower-triangular part of the coupling matrix is masked. Defaults to None. </li> </ul> <p></p>"},{"location":"api/adabmDCA.io/#function-load_params_oldformat","title":"function <code>load_params_oldformat</code>","text":"<pre><code>load_params_oldformat(\n    fname: str,\n    device: device,\n    dtype: dtype = torch.float32\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Import the parameters of the model from a file. Assumes the old DCA format. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path of the file that stores the parameters. </li> <li><code>device</code> (torch.device):  Device where to store the parameters. </li> <li><code>dtype</code> (torch.dtype):  Data type of the parameters. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> </ul> <p></p>"},{"location":"api/adabmDCA.io/#function-save_params_oldformat","title":"function <code>save_params_oldformat</code>","text":"<pre><code>save_params_oldformat(\n    fname: str,\n    params: Dict[str, Tensor],\n    mask: Optional[Tensor] = None\n) \u2192 None\n</code></pre> <p>Saves the parameters of the model in a file. Assumes the old DCA format. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the parameters. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>mask</code> (Optional[torch.Tensor]):  Tensor of shape (L, q, L, q) - Mask of the coupling matrix that determines which are the non-zero entries.  If None, the lower-triangular part of the coupling matrix is masked. Defaults to None. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.plot/","title":"Helper functions for plotting","text":""},{"location":"api/adabmDCA.plot/#module-adabmdcaplot","title":"module <code>adabmDCA.plot</code>","text":""},{"location":"api/adabmDCA.plot/#function-plot_pca","title":"function <code>plot_PCA</code>","text":"<pre><code>plot_PCA(\n    fig: Figure,\n    data1: ndarray,\n    pc1: int = 0,\n    pc2: int = 1,\n    data2: Optional[ndarray] = None,\n    labels: Union[List[str], str] = 'Data',\n    colors: Union[List[str], str] = 'black',\n    title: Optional[str] = None\n) \u2192 Figure\n</code></pre> <p>Makes the scatter plot of the components (pc1, pc2) of the input data and shows the histograms of the components. </p> <p>Args:</p> <ul> <li><code>fig</code> (Figure):  Figure to plot the data. </li> <li><code>data1</code> (np.ndarray):  Data to plot. </li> <li><code>pc1</code> (int, optional):  First principal direction. Defaults to 0. </li> <li><code>pc2</code> (int, optional):  Second principal direction. Defaults to 1. </li> <li><code>data2</code> (Optional[np.ndarray], optional):  Data to be superimposed to data1. Defaults to None. </li> <li><code>labels</code> (Union[List[str], str], optional):  Labels to put in the legend. Defaults to \"Data\". </li> <li><code>colors</code> (Union[List[str], str], optional):  Colors to be used. Defaults to \"black\". </li> <li><code>title</code> (Optional[str], optional):  Title of the plot. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Figure</code>:  Updated figure. </li> </ul> <p></p>"},{"location":"api/adabmDCA.plot/#function-plot_pearson_sampling","title":"function <code>plot_pearson_sampling</code>","text":"<pre><code>plot_pearson_sampling(\n    ax: Axes,\n    checkpoints: ndarray,\n    pearsons: ndarray,\n    pearson_training: Optional[float] = None\n) \u2192 Axes\n</code></pre> <p>Plots the Pearson correlation coefficient over sampling time. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the data. </li> <li><code>checkpoints</code> (np.ndarray):  Checkpoints of the sampling. </li> <li><code>pearsons</code> (np.ndarray):  Pearson correlation coefficients at different checkpoints. </li> <li><code>pearson_training</code> (Optional[float], optional):  Pearson correlation coefficient obtained during training. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/adabmDCA.plot/#function-plot_autocorrelation","title":"function <code>plot_autocorrelation</code>","text":"<pre><code>plot_autocorrelation(\n    ax: Axes,\n    checkpoints: ndarray,\n    autocorr: ndarray,\n    gen_seqid: float,\n    data_seqid: float\n) \u2192 Axes\n</code></pre> <p>Plots the time-autocorrelation curve of the sequence identity and the generated and data sequence identities. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the data. </li> <li><code>checkpoints</code> (np.ndarray):  Checkpoints of the sampling. </li> <li><code>autocorr</code> (np.ndarray):  Time-autocorrelation of the sequence identity. </li> <li><code>gen_seqid</code> (float):  Sequence identity of the generated data. </li> <li><code>data_seqid</code> (float):  Sequence identity of the data. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/adabmDCA.plot/#function-plot_scatter_correlations","title":"function <code>plot_scatter_correlations</code>","text":"<pre><code>plot_scatter_correlations(\n    ax: Tuple[Axes, Axes],\n    Cij_data: ndarray,\n    Cij_gen: ndarray,\n    Cijk_data: ndarray,\n    Cijk_gen: ndarray,\n    pearson_Cij: float,\n    pearson_Cijk: float\n) \u2192 Tuple[Axes, Axes]\n</code></pre> <p>Plots the scatter plot of the data and generated Cij and Cijk values. </p> <p>Args:</p> <ul> <li><code>ax</code> (Tuple[Axes, Axes]):  Tuple of 2 Axes to plot the data. </li> <li><code>Cij_data</code> (np.ndarray):  Data Cij values. </li> <li><code>Cij_gen</code> (np.ndarray):  Generated Cij values. </li> <li><code>Cijk_data</code> (np.ndarray):  Data Cijk values. </li> <li><code>Cijk_gen</code> (np.ndarray):  Generated Cijk values. </li> <li><code>pearson_Cij</code> (float):  Pearson correlation coefficient of Cij. </li> <li><code>pearson_Cijk</code> (float):  Pearson correlation coefficient of Cijk. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[Axes, Axes]</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/adabmDCA.plot/#function-plot_contact_map","title":"function <code>plot_contact_map</code>","text":"<pre><code>plot_contact_map(ax: Axes, cm: ndarray, title: Optional[str] = None) \u2192 Axes\n</code></pre> <p>Plots the contact map. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the contact map. </li> <li><code>cm</code> (np.ndarray):  Contact map to plot. </li> <li><code>title</code> (Optional[str], optional):  Title of the plot. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.resampling/","title":"adabmDCA.resampling","text":""},{"location":"api/adabmDCA.resampling/#module-adabmdcaresampling","title":"module <code>adabmDCA.resampling</code>","text":""},{"location":"api/adabmDCA.resampling/#function-compute_mixing_time","title":"function <code>compute_mixing_time</code>","text":"<pre><code>compute_mixing_time(\n    sampler: Callable[, Tensor],\n    data: Tensor,\n    params: Dict[str, Tensor],\n    n_max_sweeps: int,\n    beta: float\n) \u2192 Dict[str, List[Union[float, int]]]\n</code></pre> <p>Computes the mixing time using the t and t/2 method. The sampling will halt when the mixing time is reached or the limit of <code>n_max_sweeps</code> sweeps is reached. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function. </li> <li><code>data</code> (torch.Tensor):  Initial data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters for the sampling.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>n_max_sweeps</code> (int):  Maximum number of sweeps. </li> <li><code>beta</code> (float):  Inverse temperature for the sampling. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, List[Union[float, int]]]</code>:  Results of the mixing time analysis.          - \"seqid_t\": List of average sequence identities at time t.          - \"std_seqid_t\": List of standard deviations of sequence identities at time t.          - \"seqid_t_t_half\": List of average sequence identities between t and t/2.          - \"std_seqid_t_t_half\": List of standard deviations of sequence identities between t and t/2.          - \"t_half\": List of t/2 values (integers). </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.sampling/","title":"Sampling functions","text":""},{"location":"api/adabmDCA.sampling/#module-adabmdcasampling","title":"module <code>adabmDCA.sampling</code>","text":""},{"location":"api/adabmDCA.sampling/#function-sampling_profile","title":"function <code>sampling_profile</code>","text":"<pre><code>sampling_profile(params: Dict[str, Tensor], nsamples: int, beta: float) \u2192 Tensor\n</code></pre> <p>Samples from the profile model defined by the local biases only. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases. </li> <li><code>nsamples</code> (int):  Number of samples to generate. </li> <li><code>beta</code> (float):  Inverse temperature. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Sampled one-hot encoded sequences of shape (nsamples, L, q). </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-gibbs_step_uniform_sites","title":"function <code>gibbs_step_uniform_sites</code>","text":"<pre><code>gibbs_step_uniform_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Gibbs sampler. In this version, the mutation is attempted at the same sites for all chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-gibbs_step_independent_sites","title":"function <code>gibbs_step_independent_sites</code>","text":"<pre><code>gibbs_step_independent_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Gibbs sampler. This version selects different random sites for each chain. It is less efficient than the 'gibbs_step_uniform_sites' function, but it is more suitable for mutating starting from the same wild-type sequence since mutations are independent across chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-gibbs_sampling","title":"function <code>gibbs_sampling</code>","text":"<pre><code>gibbs_sampling(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Gibbs sampling. Attempts L * nsweeps mutations to each sequence in 'chains'. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  Initial one-hot encoded samples of size (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>nsweeps</code> (int):  Number of sweeps, where one sweep corresponds to attempting L mutations. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-metropolis_step_uniform_sites","title":"function <code>metropolis_step_uniform_sites</code>","text":"<pre><code>metropolis_step_uniform_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Metropolis sampler. In this version, the mutation is attempted at the same sites for all chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-metropolis_step_independent_sites","title":"function <code>metropolis_step_independent_sites</code>","text":"<pre><code>metropolis_step_independent_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Metropolis sampler. This version selects different random sites for each chain. It is less efficient than the 'metropolis_step_uniform_sites' function, but it is more suitable for mutating starting from the same wild-type sequence since mutations are independent across chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-metropolis_sampling","title":"function <code>metropolis_sampling</code>","text":"<pre><code>metropolis_sampling(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Metropolis sampling. Attempts L * nsweeps mutations to each sequence in 'chains'. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> <li><code>nsweeps</code> (int):  Number of sweeps to be performed, where one sweep corresponds to attempting L mutations. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/adabmDCA.sampling/#function-get_sampler","title":"function <code>get_sampler</code>","text":"<pre><code>get_sampler(sampling_method: str) \u2192 Callable\n</code></pre> <p>Returns the sampling function corresponding to the chosen method. </p> <p>Args:</p> <ul> <li><code>sampling_method</code> (str):  String indicating the sampling method. Choose between 'metropolis' and 'gibbs'. </li> </ul> <p>Raises:</p> <ul> <li><code>KeyError</code>:  Unknown sampling method. </li> </ul> <p>Returns:</p> <ul> <li><code>Callable</code>:  Sampling function. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.statmech/","title":"Statistical mechanics functions","text":""},{"location":"api/adabmDCA.statmech/#module-adabmdcastatmech","title":"module <code>adabmDCA.statmech</code>","text":""},{"location":"api/adabmDCA.statmech/#function-compute_energy","title":"function <code>compute_energy</code>","text":"<pre><code>compute_energy(x: Tensor, params: Dict[str, Tensor]) \u2192 Tensor\n</code></pre> <p>Compute the DCA energy for a batch of sequences. </p> <p>Args:</p> <ul> <li><code>x</code> (torch.Tensor):  Tensor of shape (batch_size, L, q) - batch of one-hot encoded sequences. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model.          - \"bias\": Tensor of shape (L, q) - local biases.          - \"coupling_matrix\": Tensor of shape (L, q, L, q) - coupling matrix. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Tensor of shape (batch_size,) - DCA energy for each sequence in the batch. </li> </ul> <p></p>"},{"location":"api/adabmDCA.statmech/#function-compute_log_likelihood","title":"function <code>compute_log_likelihood</code>","text":"<pre><code>compute_log_likelihood(\n    fi: Tensor,\n    fij: Tensor,\n    params: Dict[str, Tensor],\n    logZ: float\n) \u2192 float\n</code></pre> <p>Compute the log-likelihood of the model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-site frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-site frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>logZ</code> (float):  Log-partition function of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Log-likelihood of the model. </li> </ul> <p></p>"},{"location":"api/adabmDCA.statmech/#function-enumerate_states","title":"function <code>enumerate_states</code>","text":"<pre><code>enumerate_states(L: int, q: int, device: device = device(type='cpu')) \u2192 Tensor\n</code></pre> <p>Enumerate all possible states of a system of L sites and q states. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Number of sites. </li> <li><code>q</code> (int):  Number of states. </li> <li><code>device</code> (torch.device, optional):  Device to store the states. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  All possible states. </li> </ul> <p></p>"},{"location":"api/adabmDCA.statmech/#function-compute_logz_exact","title":"function <code>compute_logZ_exact</code>","text":"<pre><code>compute_logZ_exact(all_states: Tensor, params: Dict[str, Tensor]) \u2192 float\n</code></pre> <p>Compute the log-partition function of the model. </p> <p>Args:</p> <ul> <li><code>all_states</code> (torch.Tensor):  All possible states of the system. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Log-partition function of the model. </li> </ul> <p></p>"},{"location":"api/adabmDCA.statmech/#function-compute_entropy","title":"function <code>compute_entropy</code>","text":"<pre><code>compute_entropy(chains: Tensor, params: Dict[str, Tensor], logZ: float) \u2192 float\n</code></pre> <p>Compute the entropy of the DCA model. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  Chains that are supposed to be an equilibrium realization of the model. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>logZ</code> (float):  Log-partition function of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Entropy of the model. </li> </ul> <p></p>"},{"location":"api/adabmDCA.statmech/#function-iterate_tap","title":"function <code>iterate_tap</code>","text":"<pre><code>iterate_tap(\n    mag: Tensor,\n    params: Dict[str, Tensor],\n    max_iter: int = 500,\n    epsilon: float = 0.0001\n) \u2192 Tensor\n</code></pre> <p>Iterates the TAP equations until convergence. </p> <p>Args:</p> <ul> <li><code>mag</code> (torch.Tensor):  Initial magnetizations. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>max_iter</code> (int, optional):  Maximum number of iterations. Defaults to 500. </li> <li><code>epsilon</code> (float, optional):  Convergence threshold. Defaults to 1e-4. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Fixed point magnetizations of the TAP equations. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.stats/","title":"Compute statistics","text":""},{"location":"api/adabmDCA.stats/#module-adabmdcastats","title":"module <code>adabmDCA.stats</code>","text":""},{"location":"api/adabmDCA.stats/#function-get_freq_single_point","title":"function <code>get_freq_single_point</code>","text":"<pre><code>get_freq_single_point(\n    data: Tensor,\n    weights: Optional[Tensor] = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the single point frequencies of the input MSA. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  One-hot encoded data array. </li> <li><code>weights</code> (Optional[torch.Tensor], optional):  Weights of the sequences. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count to be added to the frequencies. Defaults to 0.0. </li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>:  If the input data is not a 3D tensor. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Single point frequencies. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-get_freq_two_points","title":"function <code>get_freq_two_points</code>","text":"<pre><code>get_freq_two_points(\n    data: Tensor,\n    weights: Optional[Tensor] = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the 2-points statistics of the input MSA. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  One-hot encoded data array. </li> <li><code>weights</code> (Optional[torch.Tensor], optional):  Array of weights to assign to the sequences of shape. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count for the single and two points statistics. Acts as a regularization. Defaults to 0.0. </li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>:  If the input data is not a 3D tensor. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Matrix of two-point frequencies of shape (L, q, L, q). </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-generate_unique_triplets","title":"function <code>generate_unique_triplets</code>","text":"<pre><code>generate_unique_triplets(\n    L: int,\n    ntriplets: int,\n    device: device = device(type='cpu')\n) \u2192 Tensor\n</code></pre> <p>Generates a set of unique triplets of positions. Used to compute the 3-points statistics. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Length of the sequences. </li> <li><code>ntriplets</code> (int):  Number of triplets to be generated. </li> <li><code>device</code> (torch.device, optional):  Device to perform computations on. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Tensor of shape (ntriplets, 3) containing the indices of the triplets. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-get_freq_three_points","title":"function <code>get_freq_three_points</code>","text":"<pre><code>get_freq_three_points(\n    nat: Tensor,\n    gen: Tensor,\n    ntriplets: int,\n    weights: Optional[Tensor] = None,\n    device: device = device(type='cpu')\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Computes the 3-body connected correlation statistics of the input MSAs. </p> <p>Args:</p> <ul> <li><code>nat</code> (torch.Tensor):  Input MSA representing natural data in one-hot encoding. </li> <li><code>gen</code> (torch.Tensor):  Input MSA representing generated data in one-hot encoding. </li> <li><code>ntriplets</code> (int):  Number of triplets to test. </li> <li><code>weights</code> (Optional[torch.Tensor], optional):  Importance weights for the natural sequences. Defaults to None. </li> <li><code>device</code> (torch.device, optional):  Device to perform computations on. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Natural and generated 3-points connected correlation for ntriplets randomly extracted triplets. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-get_covariance_matrix","title":"function <code>get_covariance_matrix</code>","text":"<pre><code>get_covariance_matrix(\n    data: Tensor,\n    weights: Optional[Tensor] = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the weighted covariance matrix of the input multi sequence alignment. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Input MSA in one-hot variables. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Importance weights of the sequences. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count. Defaults to 0.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Covariance matrix. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-extract_cij_from_freq","title":"function <code>extract_Cij_from_freq</code>","text":"<pre><code>extract_Cij_from_freq(\n    fij: Tensor,\n    pij: Tensor,\n    fi: Tensor,\n    pi: Tensor,\n    mask: Optional[Tensor] = None\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Extracts the lower triangular part of the covariance matrices of the natural data and generated data starting from the frequencies. </p> <p>Args:</p> <ul> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the natural data. </li> <li><code>pij</code> (torch.Tensor):  Two-point frequencies of the generated data. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the natural data. </li> <li><code>pi</code> (torch.Tensor):  Single-point frequencies of the generated data. </li> <li><code>mask</code> (Optional[torch.Tensor], optional):  Mask for comparing just a subset of the couplings. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Extracted covariance matrix entries of the natural data and generated data. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-extract_cij_from_seqs","title":"function <code>extract_Cij_from_seqs</code>","text":"<pre><code>extract_Cij_from_seqs(\n    data: Tensor,\n    chains: Tensor,\n    weights: Optional[Tensor] = None,\n    pseudo_count: float = 0.0,\n    mask: Optional[Tensor] = None\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Extracts the lower triangular part of the covariance matrices of the natural data and generated data starting from the sequences. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Natural data sequences. </li> <li><code>chains</code> (torch.Tensor):  Generated data sequences. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Weights of the sequences. Defaults to None. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count for the single and two points statistics. Acts as a regularization. Defaults to 0.0. </li> <li><code>mask</code> (torch.Tensor | None, optional):  Mask for comparing just a subset of the couplings. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Two-point frequencies of the natural data and generated data. </li> </ul> <p></p>"},{"location":"api/adabmDCA.stats/#function-get_correlation_two_points","title":"function <code>get_correlation_two_points</code>","text":"<pre><code>get_correlation_two_points(\n    fij: Tensor,\n    pij: Tensor,\n    fi: Tensor,\n    pi: Tensor,\n    mask: Optional[Tensor] = None\n) \u2192 Tuple[float, float]\n</code></pre> <p>Computes the Pearson coefficient and the slope between the two-point frequencies of data and chains. </p> <p>Args:</p> <ul> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the natural data. </li> <li><code>pij</code> (torch.Tensor):  Two-point frequencies of the generated data. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the natural data. </li> <li><code>pi</code> (torch.Tensor):  Single-point frequencies of the generated data. </li> <li><code>mask</code> (Optional[torch.Tensor], optional):  Mask to select the couplings to use for the correlation coefficient. Defaults to None.  </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[float, float]</code>:  Pearson correlation coefficient of the two-sites statistics and slope of the interpolating line. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.training/","title":"Training functions","text":""},{"location":"api/adabmDCA.training/#module-adabmdcatraining","title":"module <code>adabmDCA.training</code>","text":""},{"location":"api/adabmDCA.training/#function-update_params","title":"function <code>update_params</code>","text":"<pre><code>update_params(\n    fi: Tensor,\n    fij: Tensor,\n    pi: Tensor,\n    pij: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    lr: float\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Updates the parameters of the model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-points frequencies of the data. </li> <li><code>pi</code> (torch.Tensor):  Single-point marginals of the model. </li> <li><code>pij</code> (torch.Tensor):  Two-points marginals of the model. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the interaction graph. </li> <li><code>lr</code> (float):  Learning rate. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Updated parameters. </li> </ul> <p></p>"},{"location":"api/adabmDCA.training/#function-train_graph","title":"function <code>train_graph</code>","text":"<pre><code>train_graph(\n    sampler: Callable,\n    chains: Tensor,\n    mask: Tensor,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    lr: float,\n    max_epochs: int,\n    target_pearson: float,\n    fi_test: Optional[Tensor] = None,\n    fij_test: Optional[Tensor] = None,\n    checkpoint: Optional[Checkpoint] = None,\n    check_slope: bool = False,\n    log_weights: Optional[Tensor] = None,\n    progress_bar: bool = True,\n    *args,\n    **kwargs\n) \u2192 Tuple[Tensor, Dict[str, Tensor], Tensor, Dict[str, List[float]]]\n</code></pre> <p>Trains the model on a given graph until the target Pearson correlation is reached or the maximum number of epochs is exceeded. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function. </li> <li><code>chains</code> (torch.Tensor):  Markov chains simulated with the model. </li> <li><code>mask</code> (torch.Tensor):  Mask encoding the sparse graph. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>nsweeps</code> (int):  Number of Gibbs steps for each gradient estimation. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>max_epochs</code> (int):  Maximum number of gradient updates to be done. </li> <li><code>target_pearson</code> (float):  Target Pearson coefficient. </li> <li><code>fi_test</code> (Optional[torch.Tensor], optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (Optional[torch.Tensor], optional):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Optional[Checkpoint], optional):  Checkpoint class to be used for saving the model. Defaults to None. </li> <li><code>check_slope</code> (bool, optional):  Whether to take into account the slope for the convergence criterion or not. Defaults to False. </li> <li><code>log_weights</code> (Optional[torch.Tensor], optional):  Log-weights used for the online computation of the log-likelihood. Defaults to None. </li> <li><code>progress_bar</code> (bool, optional):  Whether to display a progress bar or not. Defaults to True. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]</code>:  Updated chains and parameters, log-weights for the log-likelihood computation. </li> </ul> <p></p>"},{"location":"api/adabmDCA.training/#function-train_eadca","title":"function <code>train_eaDCA</code>","text":"<pre><code>train_eaDCA(\n    sampler: Callable,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    chains: Tensor,\n    log_weights: Tensor,\n    target_pearson: float,\n    nsweeps: int,\n    max_epochs: int,\n    pseudo_count: float,\n    lr: float,\n    factivate: float,\n    gsteps: int,\n    fi_test: Optional[Tensor] = None,\n    fij_test: Optional[Tensor] = None,\n    checkpoint: Optional[Checkpoint] = None,\n    *args,\n    **kwargs\n) \u2192 Tuple[Tensor, Dict[str, Tensor], Tensor, Dict[str, List[float]]]\n</code></pre> <p>Fits an eaDCA model on the training data and saves the results in a file. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function to be used. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Initialization of the model's parameters. </li> <li><code>mask</code> (torch.Tensor):  Initialization of the coupling matrix's mask. </li> <li><code>chains</code> (torch.Tensor):  Initialization of the Markov chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log-weights of the chains. Used to estimate the log-likelihood. </li> <li><code>target_pearson</code> (float):  Pearson correlation coefficient on the two-points statistics to be reached. </li> <li><code>nsweeps</code> (int):  Number of Monte Carlo steps to update the state of the model. </li> <li><code>max_epochs</code> (int):  Maximum number of epochs to be performed. </li> <li><code>pseudo_count</code> (float):  Pseudo count for the single and two points statistics. Acts as a regularization. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>factivate</code> (float):  Fraction of inactive couplings to activate at each step. </li> <li><code>gsteps</code> (int):  Number of gradient updates to be performed on a given graph. </li> <li><code>fi_test</code> (Optional[torch.Tensor], optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (Optional[torch.Tensor], optional):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Optional[Checkpoint], optional):  Checkpoint class to be used to save the model. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]</code>:  Updated chains and parameters, log-weights for the log-likelihood computation, and training history. </li> </ul> <p></p>"},{"location":"api/adabmDCA.training/#function-train_eddca","title":"function <code>train_edDCA</code>","text":"<pre><code>train_edDCA(\n    sampler: Callable,\n    chains: Tensor,\n    log_weights: Tensor,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    lr: float,\n    nsweeps: int,\n    target_pearson: float,\n    target_density: float,\n    drate: float,\n    checkpoint: Optional[Checkpoint] = None,\n    fi_test: Optional[Tensor] = None,\n    fij_test: Optional[Tensor] = None,\n    *args,\n    **kwargs\n) \u2192 Tuple[Tensor, Dict[str, Tensor], Tensor, Dict[str, List[float]]]\n</code></pre> <p>Fits an edDCA model on the training data and saves the results in a file. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function to be used. </li> <li><code>chains</code> (torch.Tensor):  Initialization of the Markov chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log-weights of the chains. Used to estimate the log-likelihood. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Initialization of the model's parameters. </li> <li><code>mask</code> (torch.Tensor):  Initialization of the coupling matrix's mask. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>nsweeps</code> (int):  Number of Monte Carlo steps to update the state of the model. </li> <li><code>target_pearson</code> (float):  Pearson correlation coefficient on the two-points statistics to be reached. </li> <li><code>target_density</code> (float):  Target density of the coupling matrix. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> <li><code>checkpoint</code> (Optional[Checkpoint], optional):  Checkpoint class to be used to save the model. Defaults to None. </li> <li><code>fi_test</code> (Optional[torch.Tensor], optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (Optional[torch.Tensor], optional):  Two-point frequencies of the test data. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]</code>:  Updated chains and parameters, log-weights for the log-likelihood computation, and training history. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/adabmDCA.utils/","title":"Utilities","text":""},{"location":"api/adabmDCA.utils/#module-adabmdcautils","title":"module <code>adabmDCA.utils</code>","text":""},{"location":"api/adabmDCA.utils/#function-init_parameters","title":"function <code>init_parameters</code>","text":"<pre><code>init_parameters(fi: Tensor) \u2192 Dict[str, Tensor]\n</code></pre> <p>Initialize the parameters of the DCA model. The bias terms are initialized from the single-point frequencies 'fi', while the coupling matrix is initialized to zero. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> </ul> <p>Returns:  Dict[str, torch.Tensor]:  - <code>\"bias\" (torch.Tensor)</code>:  Bias terms.   - <code>\"coupling_matrix\" (torch.Tensor)</code>:  Coupling matrix. </p> <p></p>"},{"location":"api/adabmDCA.utils/#function-init_chains","title":"function <code>init_chains</code>","text":"<pre><code>init_chains(\n    num_chains: int,\n    L: int,\n    q: int,\n    device: device,\n    dtype: dtype = torch.float32,\n    fi: Optional[Tensor] = None\n) \u2192 Tensor\n</code></pre> <p>Initialize the Markov chains of the DCA model. If 'fi' is provided, the chains are sampled from the profile model, otherwise they are sampled uniformly at random. </p> <p>Args:</p> <ul> <li><code>num_chains</code> (int):  Number of parallel chains. </li> <li><code>L</code> (int):  Length of the MSA. </li> <li><code>q</code> (int):  Number of values that each residue can assume. </li> <li><code>device</code> (torch.device):  Device where to store the chains. </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the chains. Defaults to torch.float32. </li> <li><code>fi</code> (Optional[torch.Tensor], optional):  Single-point frequencies. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Initialized Markov chains in one-hot encoding format, shape (num_chains, L, q). </li> </ul> <p></p>"},{"location":"api/adabmDCA.utils/#function-get_mask_save","title":"function <code>get_mask_save</code>","text":"<pre><code>get_mask_save(L: int, q: int, device: device) \u2192 Tensor\n</code></pre> <p>Returns the mask to save the upper-triangular part of the coupling matrix. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Length of the MSA. </li> <li><code>q</code> (int):  Number of values that each residue can assume. </li> <li><code>device</code> (torch.device):  Device where to store the mask. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Mask. </li> </ul> <p></p>"},{"location":"api/adabmDCA.utils/#function-resample_sequences","title":"function <code>resample_sequences</code>","text":"<pre><code>resample_sequences(data: Tensor, weights: Tensor, nextract: int) \u2192 Tensor\n</code></pre> <p>Extracts nextract sequences from data with replacement according to the weights. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Data array. </li> <li><code>weights</code> (torch.Tensor):  Weights of the sequences. </li> <li><code>nextract</code> (int):  Number of sequences to be extracted. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Extracted sequences. </li> </ul> <p></p>"},{"location":"api/adabmDCA.utils/#function-get_device","title":"function <code>get_device</code>","text":"<pre><code>get_device(device: str, message: bool = True) \u2192 device\n</code></pre> <p>Returns the device where to store the tensors. </p> <p>Args:</p> <ul> <li><code>device</code> (str):  Device to be used. Possible values are 'cpu', 'cuda', 'mps'. </li> <li><code>message</code> (bool, optional):  Print the device. Defaults to True. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.device</code>:  Device. </li> </ul> <p></p>"},{"location":"api/adabmDCA.utils/#function-get_dtype","title":"function <code>get_dtype</code>","text":"<pre><code>get_dtype(dtype: str) \u2192 dtype\n</code></pre> <p>Returns the data type of the tensors. </p> <p>Args:</p> <ul> <li><code>dtype</code> (str):  Data type. Possible values are 'float32' and 'float64'. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.dtype</code>:  Data type. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"}]}