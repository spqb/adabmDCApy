{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the <code>adabmDCA 2.0</code> Documentation","text":"<p><code>adabmDCA</code> is a versatile library for Direct Coupling Analysis (DCA), enabling the training, sampling, and application of Boltzmann Machines (Potts models) on biological sequence data.</p> <p>Instructons</p> <p>This documentation is meant for providing a user-friendly description of the <code>adabmDCA</code> package main features. It is supported by:</p> <ul> <li>The main article [Rosset et al., 2025], with detailed explanations of the main features. The present documentation is a shorter version of the paper, but it includes additional features</li> <li>The Colab notebook providing a tutorial of the APIs for training, sampling and analyzing a <code>bmDCA</code> model (Python only)</li> </ul> <p>This tutorial introduces the new and enhanced version of <code>adabmDCA</code> [Muntoni at al., 2021]. The software is available in three language-specific implementations:</p> <ul> <li>C++ \u2013 optimized for single-core CPUs  </li> <li>Julia \u2013 ideal for multi-core CPU setups  </li> <li>Python \u2013 GPU-accelerated and feature-rich</li> </ul> <p>All versions share a unified terminal-based interface, allowing users to choose based on their hardware and performance needs.</p>"},{"location":"#core-capabilities","title":"Core Capabilities","text":""},{"location":"#model-training","title":"\ud83e\udde0 Model Training","text":"<p>Choose from three training strategies to fit your model complexity and goals:</p> <ul> <li><code>bmDCA</code>: Fully-connected Boltzmann Machine [Figliuzzi et al., 2018]</li> <li><code>eaDCA</code>: Sparse model with progressively added couplings [Calvanese et al., 2024]</li> <li><code>edDCA</code>: Prunes an existing <code>bmDCA</code> model down to a sparse network [Barrat-Charlaix et al., 2021]</li> </ul>"},{"location":"#applications-of-pretrained-models","title":"\u2699\ufe0f Applications of Pretrained Models","text":"<p>Once trained, models can be used to:</p> <ul> <li>Generate new sequences</li> <li>Predict structural contacts [Ekeberg et al., 2013].</li> <li>Score sequence datasets based on model energy</li> <li>Build mutational libraries with DCA-based scoring</li> </ul>"},{"location":"#advanced-features-in-python-adabmdcapy","title":"\ud83d\ude80 Advanced Features in Python (<code>adabmDCApy</code>)","text":"<p>The Python version includes exclusive features:</p> <ul> <li>Experimental feedback reintegration for refined models [Calvanese et al., 2025]</li> <li>Thermodynamic integration to estimate model entropy</li> <li><code>Profmark</code>: GPU-accelerated dataset splitting with phylogenetic and sampling bias control, based on the <code>cobalt</code> algorithm [Petti et al., 2022]</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<p>Ready to run? Skip ahead to the Quicklist for command-line usage examples.</p>"},{"location":"applications/","title":"Applications","text":"<p>Info</p> <p>We report in the Script arguments section the list of all the possible input arguments of each routine. The same information can be shown from the command line using:</p> <p><code>adabmDCA &lt;routine_name&gt; -h</code></p>"},{"location":"applications/#generate-sequences","title":"\ud83e\uddec Generate Sequences","text":"<p>Once a model is trained, it can be used to generate new sequences with:</p> <pre><code>adabmDCA sample -p &lt;path_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li><code>&lt;output_folder&gt;</code>: directory to save the output.</li> <li><code>&lt;num_gen&gt;</code>: number of sequences to generate.</li> </ul> <p>The tool first estimates the mixing time <code>t_mix</code> by simulating chains from the MSA. It then initializes <code>num_gen</code> Markov chains and runs <code>nmix * t_mix</code> sweeps (default <code>nmix = 2</code>) to ensure thermalization. </p> <p>\ud83d\udce6 Output Files:</p> <ul> <li>A FASTA file of generated sequences</li> <li>A log file for reproducing the mixing time surves (Fig. 3-left)</li> <li>A log file tracking the Pearson \\(C_{ij}\\) score as a function of the sampling time </li> </ul>"},{"location":"applications/#convergence-criterion","title":"Convergence Criterion","text":"<p>To ensure proper sampling, sequence identity is used to track mixing:</p> <ul> <li>\\(\\mathrm{SeqID}(t)\\) = identity between pairs of independent samples</li> <li>\\(\\mathrm{SeqID}(t, t/2)\\) = identity between the same chain at different times</li> </ul> <p>Denoting \\(\\pmb{a}_i(t)\\) the i-th sequence of the MSA at sampling time \\(t\\), these are computed as:</p> \\[     \\mathrm{SeqID}(t) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_{\\sigma(i)}(t)) \\qquad \\mathrm{SeqID}(t, t/2) = \\frac{1}{N} \\sum_{i=1}^{N} \\mathrm{SeqID}(\\pmb{a}_i(t), \\pmb{a}_i(t/2)) \\] <p>where \\(\\sigma(i)\\) is a random permutation of the index \\(i\\) and</p> \\[     \\mathrm{SeqID}(\\pmb{a}, \\pmb{b}) = \\frac{1}{L}\\sum_{i=1}^L \\delta_{a_i, b_i} \\in [0, 1] \\] <p>Convergence is assumed when \\(\\mathrm{SeqID}(t) \\cong \\mathrm{SeqID}(t, t/2)\\).</p> <p> Figure 3: Analysis of a bmDCA model. Left: measuring the mixing time of the model using \\(10^4\\) chains. The curves represent the average overlap among randomly initialized samples (dark blue) and the one among the same sequences between times \\(t\\) and \\(t/2\\) (light blue). Shaded areas represent the error of the mean. When the two curves merge, we can assume that the chains at time \\(t\\) forgot the memory of the chains at time \\(t/2\\). This point gives us an estimate of the model's mixing time, \\(t^{\\mathrm{mix}}\\).  Notice that the times start from 1, so the starting conditions are not shown. Right: Scatter plot of the entries of the Covariance matrix of the data versus that of the generated samples.</p>"},{"location":"applications/#contact-prediction","title":"\ud83d\udd17 Contact Prediction","text":"<p>One of the principal applications of the DCA models has been that of predicting a tertiary structure of a protein or RNA domain. In particular, with each pair of sites \\(i\\) and \\(j\\) in the MSA, <code>adabmDCA 2.0</code> computes a contact score that quantifies how likely the two associated positions in the chains are in contact in the three-dimensional structure. Formally, it corresponds to the average-product corrected (APC) Frobenius norms of the coupling matrices [Ekeberg et al., 2013], i.e.</p> \\[ F_{i,j}^{\\rm APC} = F_{i,j} - \\frac{\\sum_{k} F_{i,k} \\sum_{k} F_{k,j}}{\\sum_{kl} F_{k,l}}, \\quad F_{i,j} = \\sqrt{\\sum_{a,b \\neq '-'} J_{i,j}\\left(a, b \\right)^{2}} \\] <p>To compute contact scores:</p> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>Zero-sum gauge and gap symbols are handled internally.</p> <p>\ud83d\udce6 Output Files:</p> <ul> <li><code>&lt;label&gt;_frobenius.txt</code> with scores for each pair.</li> </ul>"},{"location":"applications/#sequence-scoring","title":"\ud83d\udcc9 Sequence Scoring","text":"<p>To score sequences using the DCA energy with a trained model:</p> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence is annotated with its statistical energy. Lower energies correspond to more likely (or better fitting) sequences under the model.</li> </ul>"},{"location":"applications/#single-mutant-library","title":"\ud83e\uddea Single Mutant Library","text":"<p>To simulate a mutational scan around a wild-type sequence:</p> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <p>\ud83d\udce6 Output Files:</p> <ul> <li>FASTA file where each sequence represents a single-point mutant, named by mutation and \\(\\Delta E\\) (change in energy). Example:</li> </ul> <pre><code>&gt;G27A | DCAscore: -0.6\n</code></pre> <p>Negative \\(\\Delta E\\) suggests improved fitness.</p>"},{"location":"applications/#reintegrated-dca-model-from-experiments","title":"\ud83d\udd01 Reintegrated DCA Model from Experiments","text":"<p>As described in [Calvanese et al., 2025], it is possible to train a DCA model informed with experimental feedback in order to improve the model's ability of generating functional sequences:</p> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --lambda_ &lt;lambda_value&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <p>Parameters:</p> <ul> <li><code>nat_msa</code>: MSA of natural sequences</li> <li><code>reint_msa</code>: MSA of tested sequences</li> <li><code>adj_vector</code>: a text file containing experimental results for the reintegration dataset. Each line of the file should contain <code>+1</code> or <code>-1</code>, where the i-th line corresponds to:<ul> <li><code>1</code> if the i-th sequence of the <code>reint_msa</code> passes the experimental test;</li> <li><code>-1</code> if the i-th sequence does not pass the experimental test;</li> </ul> </li> <li><code>lambda_</code>: reintegration strength (default: 1)</li> <li><code>alphabet</code>: <code>protein</code> or <code>rna</code>, sequence type</li> </ul> <p>\ud83d\udca1 Tip: It is possible to use continuous values from -1 to 1 for the <code>adj_vector</code>, depending on the performance of the sequence in the experiment. Additionally, the <code>lambda_</code> parameter can be fine-tuned to adjust the reintegration strength. If unsure, a good starting point is to use <code>lambda_</code> = 1 and \u00b11 values for the <code>adj_vector</code>.</p>"},{"location":"applications/#traintest-split-for-homologous-sequences","title":"\ud83e\udde0 Train/Test Split for Homologous Sequences","text":"<p>When dealing with a family of homologous sequences, splitting data into training and test sets has to be done carefully. There are two main reasons for this:</p> <ol> <li>Since homology introduces correlations between sequences, a simple random split would yield a test set that closely reproduces the training set on any statistical test,</li> <li>Because some regions of the sequence space are sampled more than others, the test set might contain densely populated clusters of sequences that would bias any type of assessment.</li> </ol> <p>To overcome these issues, we propose a simplified GPU-accelerated version of the <code>cobalt</code> algorithm introduced in [Petti et al., 2022]. The algorithm proceeds in two steps:</p> <ol> <li>A first train/test split is done, such that no sequence in the test set has more than <code>t1</code> fractional sequences identity with any sequences in the training set;</li> <li>The test set is pruned until any two sequences in it have fractional sequence identity that does not exceeds the value <code>t2</code>.</li> </ol> <p>Typical usage:</p> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre> <p>Required:</p> <ul> <li><code>t1</code>: max train/test identity</li> <li><code>t2</code>: max identity within test set</li> <li><code>n_trials</code>: number of trials to find best split</li> <li><code>output_prefix</code>: generates <code>&lt;output_prefix&gt;.train</code> and <code>&lt;output_prefix&gt;.test</code> files</li> <li><code>input_msa</code>: input MSA in FASTA format</li> </ul> <p>Optional:</p> <ul> <li><code>-t3</code>: max train/train identity</li> <li><code>--maxtrain</code>, <code>--maxtest</code>: size limits for train and test sets</li> <li><code>--alphabet</code>: sequence type (<code>protein</code>, <code>rna</code>, <code>dna</code>)</li> <li><code>--seed</code>: random seed (default 42)</li> <li><code>--device</code>: computation device (default <code>cuda</code>)</li> </ul>"},{"location":"installation/","title":"Installation Guide","text":"<p><code>adabmDCA</code> is available in three language-specific implementations:</p> <ul> <li>Python \u2013 optimized for GPU execution  </li> <li>Julia \u2013 designed for multi-core CPU usage  </li> <li>C++ \u2013 lightweight and single-core CPU compatible</li> </ul> <p>Follow the instructions below based on your preferred environment.</p>"},{"location":"installation/#python-gpu-oriented","title":"\ud83d\udd37 Python (GPU-oriented)","text":""},{"location":"installation/#option-1-install-from-pypi-recommended","title":"\ud83d\udd39 Option 1: Install from PyPI (Recommended)","text":"<pre><code>pip install adabmDCA\n</code></pre> <p>Fastest way to get started. This installs the latest stable release.</p>"},{"location":"installation/#option-2-install-from-github","title":"\ud83d\udd39 Option 2: Install from GitHub","text":"<p>Clone the repository and install the package locally:</p> <pre><code>git clone https://github.com/spqb/adabmDCApy.git\ncd adabmDCApy\npip install .\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCApy</p> <p>Info</p> <p>This version of the code assumes the user to be provided with a GPU. If this is not the case, we provide a Colab notebook that can be used with GPU hardware acceleration provided by Google.</p>"},{"location":"installation/#julia-multi-core-cpu","title":"\ud83d\udfe3 Julia (Multi-core CPU)","text":"<p>Make sure you\u2019ve installed Julia. Then choose one of the following:</p>"},{"location":"installation/#option-1-automatic-setup-via-shell","title":"\ud83d\udd39 Option 1: Automatic Setup via Shell","text":"<pre><code># Download main scripts\nwget -O adabmDCA.sh https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/adabmDCA.sh\nwget -O execute.jl https://raw.githubusercontent.com/spqb/adabmDCA.jl/refs/heads/main/execute.jl\nchmod +x adabmDCA.sh\n\n# Install dependencies and the package\njulia --eval 'using Pkg; Pkg.add(\"ArgParse\"); Pkg.add(PackageSpec(url=\"https://github.com/spqb/adabmDCA.jl\"))'\n</code></pre>"},{"location":"installation/#option-2-manual-setup-via-julia-repl","title":"\ud83d\udd39 Option 2: Manual Setup via Julia REPL","text":"<ol> <li>Launch Julia and run:</li> </ol> <pre><code>using Pkg\nPkg.add(url=\"https://github.com/spqb/adabmDCA.jl\")\nPkg.add(\"ArgParse\")\n</code></pre> <ol> <li>Download execution scripts:</li> </ol> <pre><code>wget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/adabmDCA.sh\nwget https://raw.githubusercontent.com/spqb/adabmDCA.jl/main/execute.jl\nchmod +x adabmDCA.sh\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCA.jl</p>"},{"location":"installation/#c-single-core-cpu","title":"\ud83d\udfe6 C++ (Single-core CPU)","text":"<p>A minimal setup with no external dependencies beyond <code>make</code>.</p>"},{"location":"installation/#installation-steps","title":"\ud83d\udd39 Installation Steps","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/spqb/adabmDCAc.git\ncd adabmDCAc/src\nmake\n</code></pre> <ol> <li>Return to the root folder and make the main script executable:</li> </ol> <pre><code>chmod +x adabmDCA.sh\n</code></pre> <ol> <li>Verify installation and available options:</li> </ol> <pre><code>./adabmDCA --help\n</code></pre> <p>\ud83d\udce6 GitHub repo: adabmDCAc</p> <p>Tip</p> <p>All implementations share a consistent command-line interface. You can switch between them based on your hardware and performance needs without learning new syntax.</p>"},{"location":"preprocessing/","title":"Input data and preprocessing","text":""},{"location":"preprocessing/#input-data-preprocessing","title":"Input Data &amp; Preprocessing","text":""},{"location":"preprocessing/#input-format","title":"\ud83d\udce5 Input Format","text":"<p><code>adabmDCA 2.0</code> takes as input a multiple sequence alignment (MSA) in FASTA format, typically of aligned protein or RNA/DNA sequences (see Fig. 1).</p> <p>The tool supports three built-in alphabets and also allows custom alphabets, as long as they match the MSA content.</p> Type Alphabet Symbols protein <code>-, A, C, D, E, F, G, H, I, K, L, M, N, P, Q, R, S, T, V, W, Y</code> RNA <code>-, A, C, G, U</code> DNA <code>-, A, C, G, T</code> <p>\ud83d\udca1 Line breaks within sequences are supported.</p> <p>Figure 1: Example of a fasta file containg the MSA. </p>"},{"location":"preprocessing/#preprocessing-steps","title":"\ud83d\udd27 Preprocessing Steps","text":"<p>The following steps are applied to every input MSA:</p> <ol> <li>Remove sequences with invalid symbols.</li> <li>Remove duplicate sequences.</li> <li>Reweight sequences to correct for phylogenetic and sampling bias (optional).</li> <li>Compute empirical statistics using a pseudocount.</li> </ol>"},{"location":"preprocessing/#sequence-reweighting","title":"\u2696\ufe0f Sequence Reweighting","text":"<p>To downweight overrepresented or phylogenetically related sequences, <code>adabmDCA</code> uses a clustering threshold (default: 80% identity). The weight for sequence \\(\\mathbf{a}^{(m)}\\) is:</p> \\[ w^{(m)} = \\frac{1}{N^{(m)}} \\] <p>where \\(N^{(m)}\\) is the number of sequences that have sequence identity with \\(\\mathbf{a}^{(m)}\\) above the clustering threshold.</p> <ul> <li>Set the threshold with <code>--clustering_seqid &lt;value&gt;</code></li> <li>Disable with <code>--no_reweighting</code></li> </ul>"},{"location":"preprocessing/#pseudocount-regularization","title":"\ud83e\uddee Pseudocount Regularization","text":"<p>A small pseudocount \\(\\alpha\\) is added to frequency estimates to prevent issues with rare or unobserved symbols:</p> <ul> <li> <p>One-site frequency:  \\(f_i(a) = (1 - \\alpha) f^{\\mathrm{data}}_i(a) + \\frac{\\alpha}{q}\\)</p> </li> <li> <p>Two-site frequency: \\(f_{ij}(a, b) = (1 - \\alpha) f^{\\mathrm{data}}_{ij}(a, b) + \\frac{\\alpha}{q^2}\\)</p> </li> </ul> <p>If not set via <code>--pseudocount</code>, the default is: $$ \\alpha = \\frac{1}{M_{\\text{eff}}}, \\quad \\text{with} \\quad M_{\\text{eff}} = \\sum_{m=1}^M w^{(m)} $$ being the effective number of sequences.</p>"},{"location":"quicklist/","title":"Quicklist","text":""},{"location":"quicklist/#list-of-the-main-routines-with-standard-arguments","title":"\u26a1 List of the main routines with standard arguments","text":"<ul> <li>\ud83e\udde0 Train a <code>bmDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Resume training of a <code>bmDCA</code> model:</li> </ul> <pre><code>adabmDCA train -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83c\udf31 Train an <code>eaDCA</code> model with default arguments:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --nsweeps 5\n</code></pre> <ul> <li>\ud83d\udd04 Resume training of an eaDCA model:</li> </ul> <pre><code>adabmDCA train -m eaDCA -d &lt;fasta_file&gt; -o &lt;output_folder&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\u2702\ufe0f Decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt; -p &lt;file_params&gt; -c &lt;file_chains&gt;\n</code></pre> <ul> <li>\ud83d\udd00 Train and decimate a bmDCA model to 2% density:</li> </ul> <pre><code>adabmDCA train -m edDCA -d &lt;fasta_file&gt;\n</code></pre> <ul> <li>\ud83e\uddec Generate sequences from a trained model:</li> </ul> <pre><code>adabmDCA sample -p &lt;file_params&gt; -d &lt;fasta_file&gt; -o &lt;output_folder&gt; --ngen &lt;num_gen&gt;\n</code></pre> <ul> <li>\ud83d\udcc9 Score a sequence set:</li> </ul> <pre><code>adabmDCA energies -d &lt;fasta_file&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83e\uddea Generate a single mutant library from a wild type:</li> </ul> <pre><code>adabmDCA DMS -d &lt;WT&gt; -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd17 Compute contact scores via Frobenius norm:</li> </ul> <pre><code>adabmDCA contacts -p &lt;file_params&gt; -o &lt;output_folder&gt;\n</code></pre> <ul> <li>\ud83d\udd01 Reintegrate DCA model from experiments:</li> </ul> <pre><code>adabmDCA reintegrate -d &lt;nat_msa&gt; -o &lt;output_folder&gt; --reint &lt;reint_msa&gt; --adj &lt;adj_vector&gt; --alphabet &lt;protein/rna&gt;\n</code></pre> <ul> <li>\ud83e\udde0 Train/test split for homologous sequences:</li> </ul> <pre><code>adabmDCA profmark -t1 &lt;t1&gt; -t2 &lt;t2&gt; --bestof &lt;n_trials&gt; &lt;output_prefix&gt; &lt;input_msa&gt;\n</code></pre>"},{"location":"script_arguments/","title":"Script Arguments","text":"<p>In this section we list all the possible command-line arguments for the main routines of <code>adabmDCA 2.0</code>.</p>"},{"location":"script_arguments/#train-a-dca-model","title":"Train a DCA model","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the dataset to be used for training the model. <code>-o, --output</code> DCA_model Path to the folder where to save the model. <code>-m, --model</code> bmDCA Type of model to be trained. Possible options are <code>bmDCA</code>, <code>eaDCA</code>, and <code>edDCA</code>. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>-p, --path_params</code> None Path to the file containing the model's parameters. Required for restoring the training. <code>-c, --path_chains</code> None Path to the FASTA file containing the model's chains. Required for restoring the training. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--lr</code> 0.05 Learning rate. <code>--nsweeps</code> 10 Number of sweeps for each gradient estimation. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--nchains</code> 10000 Number of Markov chains to run in parallel. <code>--target</code> 0.95 Pearson correlation coefficient on the two-sites statistics to be reached. <code>--nepochs</code> 50000 Maximum number of epochs allowed. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--seed</code> 0 Random seed. <code>--nthreads</code>\u00b9 1 Number of threads used in the Julia multithreaded version. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#eadca-options","title":"eaDCA options","text":"Command Default value Description <code>--gsteps</code> 10 Number of gradient updates to be performed on a given graph. <code>--factivate</code> 0.001 Fraction of inactive couplings to try to activate at each graph update."},{"location":"script_arguments/#eddca-options","title":"edDCA options","text":"Command Default value Description <code>--gsteps</code> 10 The number of gradient updates applied at each step of the graph convergence process. <code>--density</code> 0.02 Target density to be reached. <code>--drate</code> 0.01 Fraction of remaining couplings to be pruned at each decimation step."},{"location":"script_arguments/#sampling-from-a-dca-model","title":"Sampling from a DCA model","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model to sample from. <code>-d, --data</code> N/A Filename of the dataset MSA. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--ngen</code> None Number of samples to generate. <code>-l, --label</code> None A label to identify different algorithm runs. It prefixes the output files with this label. <code>-w, --weights</code> None Path to the file containing the weights of the sequences. If <code>None</code>, the weights are computed automatically. <code>--clustering_seqid</code> 0.8 Sequence identity threshold to be used for computing the sequence weights. <code>--no_reweighting</code> N/A If this flag is used, the routine assigns uniform weights to the sequences. <code>--nmeasure</code> 10000 Number of data sequences to use for computing the mixing time. The value min(<code>nmeasure</code>, len(data)) is taken. <code>--nmix</code> 2 Number of mixing times used to generate 'ngen' sequences starting from random. <code>--max_nsweeps</code> 10000 Maximum number of sweeps allowed. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--sampler</code> gibbs Sampling method to be used. Possible options are <code>gibbs</code> and <code>metropolis</code>. <code>--beta</code> 1.0 Inverse temperature to be used for the sampling. <code>--pseudocount</code> None Pseudo count for the single and two-sites statistics. Acts as a regularization. If <code>None</code>, it is set to \\(1/M_{\\mathrm{eff}}\\). <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#computing-dca-energies-of-a-msa","title":"Computing DCA energies of a MSA","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#generate-a-deep-mutational-scan-dms-from-a-wild-type","title":"Generate a Deep Mutational Scan (DMS) from a wild type","text":"Command Default value Description <code>-d, --data</code> N/A Filename of the input MSA containing the wild type. If multiple sequences are present, the first one is used. <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version."},{"location":"script_arguments/#compute-the-frobenius-contact-matrix","title":"Compute the Frobenius contact matrix","text":"Command Default value Description <code>-p, --path_params</code> N/A Path to the file containing the parameters of the DCA model. <code>-o, --output</code> N/A Path to the folder where to save the output. <code>-l, --label</code> None If provided, adds a label to the output files inside the output folder. <code>--alphabet</code> protein Type of encoding for the sequences. Choose among <code>protein</code>, <code>rna</code>, <code>dna</code>, or a user-defined string of tokens. <code>--device</code>\u00b9 cuda Device to be used between cuda (GPU) and CPU. Used in the Python version. <code>--dtype</code>\u00b9 float32 Data type to be used between float32 and float64. Used in the Python version. <p>\u00b9 Used in specific versions of the software.</p>"},{"location":"training/","title":"Training","text":""},{"location":"training/#module-training","title":"module <code>training</code>","text":""},{"location":"training/#function-update_params","title":"function <code>update_params</code>","text":"<pre><code>update_params(\n    fi: Tensor,\n    fij: Tensor,\n    pi: Tensor,\n    pij: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    lr: float\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Updates the parameters of the model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-points frequencies of the data. </li> <li><code>pi</code> (torch.Tensor):  Single-point marginals of the model. </li> <li><code>pij</code> (torch.Tensor):  Two-points marginals of the model. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the interaction graph. </li> <li><code>lr</code> (float):  Learning rate. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Updated parameters. </li> </ul> <p></p>"},{"location":"training/#function-train_graph","title":"function <code>train_graph</code>","text":"<pre><code>train_graph(\n    sampler: Callable,\n    chains: Tensor,\n    mask: Tensor,\n    fi: Tensor,\n    fij: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    lr: float,\n    max_epochs: int,\n    target_pearson: float,\n    fi_test: Tensor | None = None,\n    fij_test: Tensor | None = None,\n    checkpoint: Checkpoint | None = None,\n    check_slope: bool = False,\n    log_weights: Tensor | None = None,\n    progress_bar: bool = True\n) \u2192 Tuple[Tensor, Dict[str, Tensor], Tensor, Dict[str, List[float]]]\n</code></pre> <p>Trains the model on a given graph until the target Pearson correlation is reached or the maximum number of epochs is exceeded. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function. </li> <li><code>chains</code> (torch.Tensor):  Markov chains simulated with the model. </li> <li><code>mask</code> (torch.Tensor):  Mask encoding the sparse graph. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>nsweeps</code> (int):  Number of Gibbs steps for each gradient estimation. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>max_epochs</code> (int):  Maximum number of gradient updates to be done. </li> <li><code>target_pearson</code> (float):  Target Pearson coefficient. </li> <li><code>fi_test</code> (torch.Tensor | None, optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (torch.Tensor | None, optional):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Checkpoint | None, optional):  Checkpoint class to be used for saving the model. Defaults to None. </li> <li><code>check_slope</code> (bool, optional):  Whether to take into account the slope for the convergence criterion or not. Defaults to False. </li> <li><code>log_weights</code> (torch.Tensor, optional):  Log-weights used for the online computation of the log-likelihood. Defaults to None. </li> <li><code>progress_bar</code> (bool, optional):  Whether to display a progress bar or not. Defaults to True. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, Dict[str, torch.Tensor], torch.Tensor, Dict[str, List[float]]]</code>:  Updated chains and parameters, log-weights for the log-likelihood computation. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/","title":"Overview","text":""},{"location":"api/#api-overview","title":"API Overview","text":""},{"location":"api/#modules","title":"Modules","text":"<ul> <li><code>checkpoint</code></li> <li><code>cobalt</code></li> <li><code>dataset</code></li> <li><code>dca</code></li> <li><code>fasta</code></li> <li><code>functional</code></li> <li><code>io</code></li> <li><code>plot</code></li> <li><code>resampling</code></li> <li><code>sampling</code></li> <li><code>statmech</code></li> <li><code>stats</code></li> <li><code>utils</code></li> </ul>"},{"location":"api/#classes","title":"Classes","text":"<ul> <li><code>checkpoint.Checkpoint</code>: Helper class to save the model's parameters and chains at regular intervals during training and to log the</li> <li><code>dataset.DatasetDCA</code>: Dataset class for handling multi-sequence alignments data.</li> </ul>"},{"location":"api/#functions","title":"Functions","text":"<ul> <li><code>cobalt.prune_redundant_sequences</code>: Prunes sequences from X such that no sequence has more than 'seqid_th' fraction of its residues identical to any other sequence in the set.</li> <li><code>cobalt.run_cobalt</code>: Runs the Cobalt algorithm to split the input MSA into training and test sets.</li> <li><code>cobalt.split_train_test</code>: Splits X into two sets, T and S, such that no sequence in S has more than</li> <li><code>dca.get_contact_map</code>: Computes the contact map from the model coupling matrix.</li> <li><code>dca.get_mf_contact_map</code>: Computes the contact map from the model coupling matrix.</li> <li><code>dca.get_seqid</code>: Returns a tensor containing the sequence identities between two sets of one-hot encoded sequences.</li> <li><code>dca.get_seqid_stats</code>: - If s2 is provided, computes the mean and the standard deviation of the mean sequence identity between two sets of one-hot encoded sequences.</li> <li><code>dca.set_zerosum_gauge</code>: Sets the zero-sum gauge on the coupling matrix.</li> <li><code>fasta.compute_weights</code>: Computes the weight to be assigned to each sequence 's' in 'data' as 1 / n_clust, where 'n_clust' is the number of sequences</li> <li><code>fasta.decode_sequence</code>: Takes a numeric sequence or list of seqences in input an returns the corresponding string encoding.</li> <li><code>fasta.encode_sequence</code>: Encodes a sequence or a list of sequences into a numeric format.</li> <li><code>fasta.get_tokens</code>: Converts the alphabet into the corresponding tokens.</li> <li><code>fasta.import_from_fasta</code>: Import sequences from a fasta file. The following operations are performed:</li> <li><code>fasta.validate_alphabet</code>: Check if the chosen alphabet is compatible with the input sequences.</li> <li><code>fasta.write_fasta</code>: Generate a fasta file with the input sequences.</li> <li><code>functional.one_hot</code>: A fast one-hot encoding function faster than the PyTorch one working with torch.int32 and returning a float Tensor.</li> <li><code>io.load_chains</code>: Loads the sequences from a fasta file and returns the one-hot encoded version.</li> <li><code>io.load_params</code>: Import the parameters of the model from a file.</li> <li><code>io.load_params_oldformat</code>: Import the parameters of the model from a file. Assumes the old DCA format.</li> <li><code>io.save_chains</code>: Saves the chains in a fasta file.</li> <li><code>io.save_params</code>: Saves the parameters of the model in a file.</li> <li><code>io.save_params_oldformat</code>: Saves the parameters of the model in a file. Assumes the old DCA format.</li> <li><code>plot.plot_PCA</code>: Makes the scatter plot of the components (pc1, pc2) of the input data and shows the histograms of the components.</li> <li><code>plot.plot_autocorrelation</code>: Plots the time-autocorrelation curve of the sequence identity and the generated and data sequence identities.</li> <li><code>plot.plot_contact_map</code>: Plots the contact map.</li> <li><code>plot.plot_pearson_sampling</code>: Plots the Pearson correlation coefficient over sampling time.</li> <li><code>plot.plot_scatter_correlations</code>: Plots the scatter plot of the data and generated Cij and Cijk values.</li> <li><code>resampling.compute_mixing_time</code>: Computes the mixing time using the t and t/2 method. The sampling will halt when the mixing time is reached or</li> <li><code>sampling.get_sampler</code>: Returns the sampling function corresponding to the chosen method.</li> <li><code>sampling.gibbs_sampling</code>: Gibbs sampling.</li> <li><code>sampling.gibbs_step_independent_sites</code>: Performs a single mutation using the Gibbs sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.gibbs_step_uniform_sites</code>: Performs a single mutation using the Gibbs sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>sampling.metropolis_sampling</code>: Metropolis sampling.</li> <li><code>sampling.metropolis_step_independent_sites</code>: Performs a single mutation using the Metropolis sampler. This version selects different random sites for each chain. It is</li> <li><code>sampling.metropolis_step_uniform_sites</code>: Performs a single mutation using the Metropolis sampler. In this version, the mutation is attempted at the same sites for all chains.</li> <li><code>statmech.compute_energy</code>: Compute the DCA energy of the sequences in X.</li> <li><code>statmech.compute_entropy</code>: Compute the entropy of the DCA model.</li> <li><code>statmech.compute_logZ_exact</code>: Compute the log-partition function of the model.</li> <li><code>statmech.compute_log_likelihood</code>: Compute the log-likelihood of the model.</li> <li><code>statmech.enumerate_states</code>: Enumerate all possible states of a system of L sites and q states.</li> <li><code>statmech.iterate_tap</code>: Iterates the TAP equations until convergence.</li> <li><code>stats.extract_Cij_from_freq</code>: Extracts the lower triangular part of the covariance matrices of the data and chains starting from the frequencies.</li> <li><code>stats.extract_Cij_from_seqs</code>: Extracts the lower triangular part of the covariance matrices of the data and chains starting from the sequences.</li> <li><code>stats.generate_unique_triplets</code>: Generates a set of unique triplets of positions. Used to compute the 3-points statistics.</li> <li><code>stats.get_correlation_two_points</code>: Computes the Pearson coefficient and the slope between the two-point frequencies of data and chains.</li> <li><code>stats.get_covariance_matrix</code>: Computes the weighted covariance matrix of the input multi sequence alignment.</li> <li><code>stats.get_freq_single_point</code>: Computes the single point frequencies of the input MSA.</li> <li><code>stats.get_freq_three_points</code>: Computes the 3-body connected correlation statistics of the input MSAs.</li> <li><code>stats.get_freq_two_points</code>: Computes the 2-points statistics of the input MSA.</li> <li><code>utils.get_device</code>: Returns the device where to store the tensors.</li> <li><code>utils.get_dtype</code>: Returns the data type of the tensors.</li> <li><code>utils.get_mask_save</code>: Returns the mask to save the upper-triangular part of the coupling matrix.</li> <li><code>utils.init_chains</code>: Initialize the chains of the DCA model. If 'fi' is provided, the chains are sampled from the</li> <li><code>utils.init_parameters</code>: Initialize the parameters of the DCA model.</li> <li><code>utils.resample_sequences</code>: Extracts nextract sequences from data with replacement according to the weights.</li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/checkpoint/","title":"Checkpoint","text":""},{"location":"api/checkpoint/#module-checkpoint","title":"module <code>checkpoint</code>","text":""},{"location":"api/checkpoint/#class-checkpoint","title":"class <code>Checkpoint</code>","text":"<p>Helper class to save the model's parameters and chains at regular intervals during training and to log the progress of the training. </p> <p></p>"},{"location":"api/checkpoint/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(\n    file_paths: dict,\n    tokens: str,\n    args: dict,\n    params: Optional[Dict[str, Tensor]] = None,\n    chains: Tensor | None = None,\n    use_wandb: bool = False\n)\n</code></pre> <p>Initializes the Checkpoint class. </p> <p>Args:</p> <ul> <li><code>file_paths</code> (dict):  Dictionary containing the paths of the files to be saved. </li> <li><code>tokens</code> (str):  Alphabet to be used for encoding the sequences. </li> <li><code>args</code> (dict):  Dictionary containing the arguments of the training. </li> <li><code>params</code> (Dict[str, torch.Tensor] | None, optional):  Parameters of the model. Defaults to None. </li> <li><code>chains</code> (torch.Tensor | None, optional):  Chains. Defaults to None. </li> <li><code>use_wandb</code> (bool, optional):  Whether to use Weights &amp; Biases for logging. Defaults to False. </li> </ul> <p></p>"},{"location":"api/checkpoint/#method-check","title":"method <code>check</code>","text":"<pre><code>check(updates: int) \u2192 bool\n</code></pre> <p>Checks if a checkpoint has been reached. </p> <p>Args:</p> <ul> <li><code>updates</code> (int):  Number of gradient updates performed. </li> </ul> <p>Returns:</p> <ul> <li><code>bool</code>:  Whether a checkpoint has been reached. </li> </ul> <p></p>"},{"location":"api/checkpoint/#method-log","title":"method <code>log</code>","text":"<pre><code>log(record: Dict[str, Any]) \u2192 None\n</code></pre> <p>Adds a key-value pair to the log dictionary </p> <p>Args:</p> <ul> <li><code>record</code> (Dict[str, Any]):  Key-value pairs to be added to the log dictionary. </li> </ul> <p></p>"},{"location":"api/checkpoint/#method-save","title":"method <code>save</code>","text":"<pre><code>save(\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    chains: Tensor,\n    log_weights: Tensor\n) \u2192 None\n</code></pre> <p>Saves the chains and the parameters of the model. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the model's coupling matrix representing the interaction graph </li> <li><code>chains</code> (torch.Tensor):  Chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log of the chain weights. Used for AIS. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/cli/","title":"Cli","text":""},{"location":"api/cli/#module-cli","title":"module <code>cli</code>","text":""},{"location":"api/cli/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/cobalt/","title":"Train/test split","text":""},{"location":"api/cobalt/#module-cobalt","title":"module <code>cobalt</code>","text":""},{"location":"api/cobalt/#function-split_train_test","title":"function <code>split_train_test</code>","text":"<pre><code>split_train_test(\n    headers: ndarray,\n    X: Tensor,\n    seqid_th: float,\n    rnd_gen: Generator | None = None\n) \u2192 tuple[ndarray, Tensor, ndarray, Tensor]\n</code></pre> <p>Splits X into two sets, T and S, such that no sequence in S has more than 'seqid_th' fraction of its residues identical to any sequence in T. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA. </li> <li><code>seqid_th</code> (float):  Threshold sequence identity. </li> <li><code>rnd_gen</code> (torch.Generator, optional):  Random number generator. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>tuple[np.ndarray, torch.Tensor, np.ndarray, torch.Tensor]</code>:  Training and test sets. </li> </ul> <p></p>"},{"location":"api/cobalt/#function-prune_redundant_sequences","title":"function <code>prune_redundant_sequences</code>","text":"<pre><code>prune_redundant_sequences(\n    headers: ndarray,\n    X: Tensor,\n    seqid_th: float,\n    rnd_gen: Generator | None = None\n) \u2192 tuple[ndarray, Tensor]\n</code></pre> <p>Prunes sequences from X such that no sequence has more than 'seqid_th' fraction of its residues identical to any other sequence in the set. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA. </li> <li><code>seqid_th</code> (float):  Threshold sequence identity. </li> <li><code>rnd_gen</code> (torch.Generator, optional):  Random generator. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>tuple[np.ndarray, torch.Tensor]</code>:  Pruned sequences. </li> </ul> <p></p>"},{"location":"api/cobalt/#function-run_cobalt","title":"function <code>run_cobalt</code>","text":"<pre><code>run_cobalt(\n    headers: ndarray,\n    X: Tensor,\n    t1: float,\n    t2: float,\n    t3: float,\n    max_train: int | None,\n    max_test: int | None,\n    rnd_gen: Generator | None = None\n) \u2192 tuple[ndarray, Tensor, ndarray, Tensor]\n</code></pre> <p>Runs the Cobalt algorithm to split the input MSA into training and test sets. </p> <p>Args:</p> <ul> <li><code>headers</code> (np.ndarray):  Array of sequence headers. </li> <li><code>X</code> (torch.Tensor):  Encoded input MSA. </li> <li><code>t1</code> (float):  No sequence in S has more than this fraction of its residues identical to any sequence in T. </li> <li><code>t2</code> (float):  No pair of test sequences has more than this value fractional identity. </li> <li><code>t3</code> (float):  No pair of training sequences has more than this value fractional identity. </li> <li><code>max_train</code> (int | None):  Maximum number of sequences in the training set. </li> <li><code>max_test</code> (int | None):  Maximum number of sequences in the test set. </li> <li><code>rnd_gen</code> (torch.Generator, optional):  Random number generator. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>tuple[np.ndarray, torch.Tensor, np.ndarray, torch.Tensor]</code>:  Training and test sets. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/dataset/","title":"Dataset class","text":""},{"location":"api/dataset/#module-dataset","title":"module <code>dataset</code>","text":""},{"location":"api/dataset/#class-datasetdca","title":"class <code>DatasetDCA</code>","text":"<p>Dataset class for handling multi-sequence alignments data. </p> <p></p>"},{"location":"api/dataset/#method-__init__","title":"method <code>__init__</code>","text":"<pre><code>__init__(\n    path_data: str | Path,\n    path_weights: str | Path | None = None,\n    alphabet: str = 'protein',\n    clustering_th: float = 0.8,\n    no_reweighting: bool = False,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32,\n    message: bool = True\n)\n</code></pre> <p>Initialize the dataset. </p> <p>Args:</p> <ul> <li><code>path_data</code> (str | Path):  Path to multi sequence alignment in fasta format. </li> <li><code>path_weights</code> (str | Path | None, optional):  Path to the file containing the importance weights of the sequences. If None, the weights are computed automatically. </li> <li><code>alphabet</code> (str, optional):  Selects the type of encoding of the sequences. Default choices are (\"protein\", \"rna\", \"dna\"). Defaults to \"protein\". </li> <li><code>clustering_th</code> (float, optional):  Sequence identity threshold for clustering. Defaults to 0.8. </li> <li><code>no_reweighting</code> (bool, optional):  If True, the weights are not computed. Defaults to False. </li> <li><code>device</code> (torch.device, optional):  Device to be used. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the dataset. Defaults to torch.float32. </li> <li><code>message</code> (bool, optional):  Print the import message. Defaults to True. </li> </ul> <p></p>"},{"location":"api/dataset/#method-get_effective_size","title":"method <code>get_effective_size</code>","text":"<pre><code>get_effective_size() \u2192 int\n</code></pre> <p>Returns the effective size (Meff) of the dataset. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Effective size of the dataset. </li> </ul> <p></p>"},{"location":"api/dataset/#method-get_num_residues","title":"method <code>get_num_residues</code>","text":"<pre><code>get_num_residues() \u2192 int\n</code></pre> <p>Returns the number of residues (L) in the multi-sequence alignment. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Length of the MSA. </li> </ul> <p></p>"},{"location":"api/dataset/#method-get_num_states","title":"method <code>get_num_states</code>","text":"<pre><code>get_num_states() \u2192 int\n</code></pre> <p>Returns the number of states (q) in the alphabet. </p> <p>Returns:</p> <ul> <li><code>int</code>:  Number of states. </li> </ul> <p></p>"},{"location":"api/dataset/#method-shuffle","title":"method <code>shuffle</code>","text":"<pre><code>shuffle() \u2192 None\n</code></pre> <p>Shuffles the dataset.  </p> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/dca/","title":"DCA utils","text":""},{"location":"api/dca/#module-dca","title":"module <code>dca</code>","text":""},{"location":"api/dca/#function-get_seqid","title":"function <code>get_seqid</code>","text":"<pre><code>get_seqid(s1: Tensor, s2: Tensor | None = None) \u2192 Tensor\n</code></pre> <p>Returns a tensor containing the sequence identities between two sets of one-hot encoded sequences.  - If s2 is provided, computes the sequence identity between the corresponding sequences in s1 and s2.  - If s2 is a single sequence (L, q), it computes the sequence identities between the dataset s1 and s2.  - If s2 is none, computes the sequence identity between s1 and a permutation of s1. </p> <p>Args:</p> <ul> <li><code>s1</code> (torch.Tensor):  One-hot encoded sequence dataset 1 of shape (batch_size, L, q). </li> <li><code>s2</code> (torch.Tensor | None):  One-hot encoded sequence dataset 2 of shape (batch_size, L, q) or (L, q). Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Tensor of sequence identities. </li> </ul> <p></p>"},{"location":"api/dca/#function-get_seqid_stats","title":"function <code>get_seqid_stats</code>","text":"<pre><code>get_seqid_stats(s1: Tensor, s2: Tensor | None = None) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <ul> <li>If s2 is provided, computes the mean and the standard deviation of the mean sequence identity between two sets of one-hot encoded sequences. </li> <li>If s2 is a single sequence (L, q), it computes the mean and the standard deviation of the mean sequence identity between the dataset s1 and s2. </li> <li>If s2 is none, computes the mean and the standard deviation of the mean of the sequence identity between s1 and a permutation of s1. </li> </ul> <p>Args:</p> <ul> <li><code>s1</code> (torch.Tensor):  One-hot encoded sequence dataset 1 of shape (batch_size, L, q). </li> <li><code>s2</code> (torch.Tensor | None):  One-hot encoded sequence dataset 2 of shape (batch_size, L, q) or (L, q). Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Mean sequence identity and standard deviation of the mean. </li> </ul> <p></p>"},{"location":"api/dca/#function-set_zerosum_gauge","title":"function <code>set_zerosum_gauge</code>","text":"<pre><code>set_zerosum_gauge(params: Dict[str, Tensor]) \u2192 Dict[str, Tensor]\n</code></pre> <p>Sets the zero-sum gauge on the coupling matrix. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters with fixed gauge. </li> </ul> <p></p>"},{"location":"api/dca/#function-get_contact_map","title":"function <code>get_contact_map</code>","text":"<pre><code>get_contact_map(params: Dict[str, Tensor], tokens: str) \u2192 ndarray\n</code></pre> <p>Computes the contact map from the model coupling matrix. </p> <p>Args:</p> <ul> <li><code>params</code> (Dict[str, torch.Tensor]):  Model parameters. </li> <li><code>tokens</code> (str):  Alphabet. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Contact map. </li> </ul> <p></p>"},{"location":"api/dca/#function-get_mf_contact_map","title":"function <code>get_mf_contact_map</code>","text":"<pre><code>get_mf_contact_map(\n    data: Tensor,\n    tokens: str,\n    weights: Tensor | None = None\n) \u2192 ndarray\n</code></pre> <p>Computes the contact map from the model coupling matrix. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Input one-hot data tensor. </li> <li><code>tokens</code> (str):  Alphabet. </li> <li><code>weights</code> (torch.Tensor | None):  Weights for the data points. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Contact map. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/fasta/","title":"Fasta utils","text":""},{"location":"api/fasta/#module-fasta","title":"module <code>fasta</code>","text":""},{"location":"api/fasta/#global-variables","title":"Global Variables","text":"<ul> <li>TOKENS_PROTEIN</li> <li>TOKENS_RNA</li> <li>TOKENS_DNA</li> </ul>"},{"location":"api/fasta/#function-get_tokens","title":"function <code>get_tokens</code>","text":"<pre><code>get_tokens(alphabet: str) \u2192 str\n</code></pre> <p>Converts the alphabet into the corresponding tokens. </p> <p>Args:</p> <ul> <li><code>alphabet</code> (str):  Alphabet to be used for the encoding. It can be either \"protein\", \"rna\", \"dna\" or a custom string of tokens. </li> </ul> <p>Returns:</p> <ul> <li><code>str</code>:  Tokens of the alphabet. </li> </ul> <p></p>"},{"location":"api/fasta/#function-encode_sequence","title":"function <code>encode_sequence</code>","text":"<pre><code>encode_sequence(sequence: str | ndarray | list, tokens: str) \u2192 ndarray\n</code></pre> <p>Encodes a sequence or a list of sequences into a numeric format. </p> <p>Args:</p> <ul> <li><code>sequence</code> (str | np.ndarray | list):  Input sequence. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Returns:</p> <ul> <li><code>np.ndarray</code>:  Encoded sequence or sequences. </li> </ul> <p></p>"},{"location":"api/fasta/#function-decode_sequence","title":"function <code>decode_sequence</code>","text":"<pre><code>decode_sequence(sequence: list | ndarray | Tensor, tokens: str) \u2192 str | ndarray\n</code></pre> <p>Takes a numeric sequence or list of seqences in input an returns the corresponding string encoding. </p> <p>Args:</p> <ul> <li><code>sequence</code> (np.ndarray):  Input sequences. Can be either a 1D, 2D or a 3D (one-hot encoded) iterable. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Returns:</p> <ul> <li><code>str | np.ndarray</code>:  string or array of strings with the decoded input. </li> </ul> <p></p>"},{"location":"api/fasta/#function-import_from_fasta","title":"function <code>import_from_fasta</code>","text":"<pre><code>import_from_fasta(\n    fasta_name: str | Path,\n    tokens: str | None = None,\n    filter_sequences: bool = False,\n    remove_duplicates: bool = False\n) \u2192 Tuple[ndarray, ndarray]\n</code></pre> <p>Import sequences from a fasta file. The following operations are performed:  - If 'tokens' is provided, encodes the sequences in numeric format.  - If 'filter_sequences' is True, removes the sequences whose tokens are not present in the alphabet.  - If 'remove_duplicates' is True, removes the duplicated sequences. </p> <p>Args:</p> <ul> <li><code>fasta_name</code> (str | Path):  Path to the fasta file. </li> <li><code>tokens</code> (str | None, optional):  Alphabet to be used for the encoding. If provided, encodes the sequences in numeric format. </li> <li><code>filter_sequences</code> (bool, optional):  If True, removes the sequences whose tokens are not present in the alphabet. Defaults to False. </li> <li><code>remove_duplicates</code> (bool, optional):  If True, removes the duplicated sequences. Defaults to False. </li> </ul> <p>Raises:</p> <ul> <li><code>RuntimeError</code>:  The file is not in fasta format. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[np.ndarray, np.ndarray]</code>:  headers, sequences. </li> </ul> <p></p>"},{"location":"api/fasta/#function-write_fasta","title":"function <code>write_fasta</code>","text":"<pre><code>write_fasta(\n    fname: str,\n    headers: list | ndarray | Tensor,\n    sequences: list | ndarray | Tensor,\n    remove_gaps: bool = False,\n    tokens: str = 'protein'\n)\n</code></pre> <p>Generate a fasta file with the input sequences. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Name of the output fasta file. </li> <li><code>headers</code> (list | np.ndarray | torch.Tensor):  Iterable with sequences' headers. </li> <li><code>sequences</code> (list | np.ndarray | torch.Tensor):  Iterable with sequences in string, categorical or one-hot encoded format. </li> <li><code>remove_gaps</code> (bool, optional):  If True, removes the gap from the alignment. Defaults to False. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. Defaults to protein. </li> </ul> <p></p>"},{"location":"api/fasta/#function-compute_weights","title":"function <code>compute_weights</code>","text":"<pre><code>compute_weights(\n    data: ndarray | Tensor,\n    th: float = 0.8,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32\n) \u2192 Tensor\n</code></pre> <p>Computes the weight to be assigned to each sequence 's' in 'data' as 1 / n_clust, where 'n_clust' is the number of sequences that have a sequence identity with 's' &gt;= th. </p> <p>Args:</p> <ul> <li><code>data</code> (np.ndarray | torch.Tensor):  Input dataset. Must be either a 2D or a 3D (one-hot encoded) array. </li> <li><code>th</code> (float, optional):  Sequence identity threshold for the clustering. Defaults to 0.8. </li> <li><code>device</code> (torch.device, optional):  Device. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Array with the weights of the sequences. </li> </ul> <p></p>"},{"location":"api/fasta/#function-validate_alphabet","title":"function <code>validate_alphabet</code>","text":"<pre><code>validate_alphabet(sequences: ndarray, tokens: str)\n</code></pre> <p>Check if the chosen alphabet is compatible with the input sequences. </p> <p>Args:</p> <ul> <li><code>sequences</code> (np.ndarray):  Input sequences. </li> <li><code>tokens</code> (str):  Alphabet to be used for the encoding. </li> </ul> <p>Raises:</p> <ul> <li><code>KeyError</code>:  The chosen alphabet is incompatible with the Multi-Sequence Alignment. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/functional/","title":"Special functions","text":""},{"location":"api/functional/#module-functional","title":"module <code>functional</code>","text":""},{"location":"api/functional/#function-one_hot","title":"function <code>one_hot</code>","text":"<pre><code>one_hot(x: Tensor, num_classes: int = -1, dtype: dtype = torch.float32)\n</code></pre> <p>A fast one-hot encoding function faster than the PyTorch one working with torch.int32 and returning a float Tensor. Works only for 2D tensors. </p> <p>Args:</p> <ul> <li><code>x</code> (torch.Tensor):  Input tensor to be one-hot encoded. </li> <li><code>num_classes</code> (int, optional):  Number of classes. If -1, the number of classes is inferred from the input tensor. Defaults to -1. </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the output tensor. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  One-hot encoded tensor. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/graph/","title":"Graph","text":""},{"location":"api/graph/#module-graph","title":"module <code>graph</code>","text":""},{"location":"api/graph/#function-update_mask_activation","title":"function <code>update_mask_activation</code>","text":"<pre><code>update_mask_activation(Dkl: Tensor, mask: Tensor, nactivate: int) \u2192 Tensor\n</code></pre> <p>Updates the mask by removing the nactivate couplings with the smallest Dkl. </p> <p>Args:</p> <ul> <li><code>Dkl</code> (torch.Tensor):  Kullback-Leibler divergence matrix. </li> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>nactivate</code> (int):  Number of couplings to be activated at each graph update. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated mask. </li> </ul> <p></p>"},{"location":"api/graph/#function-update_mask_decimation","title":"function <code>update_mask_decimation</code>","text":"<pre><code>update_mask_decimation(mask: Tensor, Dkl: Tensor, drate: float) \u2192 Tensor\n</code></pre> <p>Updates the mask by removing the n_remove couplings with the smallest Dkl. </p> <p>Args:</p> <ul> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>Dkl</code> (torch.Tensor):  Kullback-Leibler divergence matrix. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated mask. </li> </ul> <p></p>"},{"location":"api/graph/#function-decimate_graph","title":"function <code>decimate_graph</code>","text":"<pre><code>decimate_graph(\n    pij: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    drate: float\n) \u2192 Tuple[Dict[str, Tensor], Tensor]\n</code></pre> <p>Performs one decimation step and updates the parameters and mask. </p> <p>Args:</p> <ul> <li><code>pij</code> (torch.Tensor):  Two-point marginal probability distribution. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[Dict[str, torch.Tensor], torch.Tensor]</code>:  Updated parameters and mask. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/io/","title":"Import and export","text":""},{"location":"api/io/#module-io","title":"module <code>io</code>","text":"<p>The io module provides the Python interfaces to stream handling. The builtin open function is defined in this module. </p> <p>At the top of the I/O hierarchy is the abstract base class IOBase. It defines the basic interface to a stream. Note, however, that there is no separation between reading and writing to streams; implementations are allowed to raise an OSError if they do not support a given operation. </p> <p>Extending IOBase is RawIOBase which deals simply with the reading and writing of raw bytes to a stream. FileIO subclasses RawIOBase to provide an interface to OS files. </p> <p>BufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its subclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer streams that are readable, writable, and both respectively. BufferedRandom provides a buffered interface to random access streams. BytesIO is a simple stream of in-memory bytes. </p> <p>Another IOBase subclass, TextIOBase, deals with the encoding and decoding of streams into text. TextIOWrapper, which extends it, is a buffered text interface to a buffered raw stream (<code>BufferedIOBase</code>). Finally, StringIO is an in-memory stream for text. </p> <p>Argument names are not part of the specification, and only the arguments of open() are intended to be used as keyword arguments. </p> <p>data: </p> <p>DEFAULT_BUFFER_SIZE </p> <p>An int containing the default buffer size used by the module's buffered  I/O classes. open() uses the file's blksize (as obtained by os.stat) if  possible. </p>"},{"location":"api/io/#global-variables","title":"Global Variables","text":"<ul> <li>DEFAULT_BUFFER_SIZE</li> <li>SEEK_SET</li> <li>SEEK_CUR</li> <li>SEEK_END</li> </ul>"},{"location":"api/io/#function-load_chains","title":"function <code>load_chains</code>","text":"<pre><code>load_chains(\n    fname: str,\n    tokens: str,\n    load_weights: bool = False,\n    device: device = device(type='cpu'),\n    dtype: dtype = torch.float32\n) \u2192 List[Tensor]\n</code></pre> <p>Loads the sequences from a fasta file and returns the one-hot encoded version. If the sequences are weighted, the log-weights are also returned. If the sequences are not weighted, the log-weights are set to 0. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file containing the sequences. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>load_weights</code> (bool, optional):  If True, the log-weights are loaded and returned. Defaults to False. </li> <li><code>device</code> (torch.device, optional):  Device where to store the sequences. Defaults to \"cpu\". </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the sequences. Defaults to torch.float32 </li> </ul> <p>Return:   - <code>List[torch.Tensor]</code>:  One-hot encoded sequences and log-weights if load_weights is True. </p> <p></p>"},{"location":"api/io/#function-save_chains","title":"function <code>save_chains</code>","text":"<pre><code>save_chains(\n    fname: str,\n    chains: Tensor | ndarray,\n    tokens: str,\n    log_weights: Tensor | ndarray | None = None\n) \u2192 None\n</code></pre> <p>Saves the chains in a fasta file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the chains. </li> <li><code>chains</code> (torch.Tensor | np.ndarray):  Chains. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>log_weights</code> (torch.Tensor | None, optional):  Log-weights of the chains. Defaults to None. </li> </ul> <p></p>"},{"location":"api/io/#function-load_params","title":"function <code>load_params</code>","text":"<pre><code>load_params(\n    fname: str,\n    tokens: str,\n    device: device,\n    dtype: dtype = torch.float32\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Import the parameters of the model from a file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path of the file that stores the parameters. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>device</code> (torch.device):  Device where to store the parameters. </li> <li><code>dtype</code> (torch.dtype):  Data type of the parameters. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model. </li> </ul> <p></p>"},{"location":"api/io/#function-save_params","title":"function <code>save_params</code>","text":"<pre><code>save_params(\n    fname: str,\n    params: Dict[str, Tensor],\n    tokens: str,\n    mask: Tensor | None = None\n) \u2192 None\n</code></pre> <p>Saves the parameters of the model in a file. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the parameters. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>tokens</code> (str):  \"protein\", \"dna\", \"rna\" or another string with the alphabet to be used. </li> <li><code>mask</code> (torch.Tensor | None):  Mask of the coupling matrix that determines which are the non-zero entries.  If None, the lower-triangular part of the coupling matrix is masked. Defaults to None. </li> </ul> <p></p>"},{"location":"api/io/#function-load_params_oldformat","title":"function <code>load_params_oldformat</code>","text":"<pre><code>load_params_oldformat(\n    fname: str,\n    device: device,\n    dtype: dtype = torch.float32\n) \u2192 Dict[str, Tensor]\n</code></pre> <p>Import the parameters of the model from a file. Assumes the old DCA format. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path of the file that stores the parameters. </li> <li><code>device</code> (torch.device):  Device where to store the parameters. </li> <li><code>dtype</code> (torch.dtype):  Data type of the parameters. Defaults to torch.float32. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model. </li> </ul> <p></p>"},{"location":"api/io/#function-save_params_oldformat","title":"function <code>save_params_oldformat</code>","text":"<pre><code>save_params_oldformat(\n    fname: str,\n    params: Dict[str, Tensor],\n    mask: Tensor | None = None\n) \u2192 None\n</code></pre> <p>Saves the parameters of the model in a file. Assumes the old DCA format. </p> <p>Args:</p> <ul> <li><code>fname</code> (str):  Path to the file where to save the parameters. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>mask</code> (torch.Tensor):  Mask of the coupling matrix that determines which are the non-zero entries.  If None, the lower-triangular part of the coupling matrix is masked. Defaults to None. </li> </ul> <p></p>"},{"location":"api/io/#class-bufferediobase","title":"class <code>BufferedIOBase</code>","text":"<p>Base class for buffered IO objects. </p> <p>The main difference with RawIOBase is that the read() method supports omitting the size argument, and does not have a default implementation that defers to readinto(). </p> <p>In addition, read(), readinto() and write() may raise BlockingIOError if the underlying raw stream is in non-blocking mode and not ready; unlike their raw counterparts, they will never return None. </p> <p>A typical implementation should not inherit from a RawIOBase implementation, but wrap one. </p> <p></p>"},{"location":"api/io/#class-iobase","title":"class <code>IOBase</code>","text":"<p>The abstract base class for all I/O classes. </p> <p>This class provides dummy implementations for many methods that derived classes can override selectively; the default implementations represent a file that cannot be read, written or seeked. </p> <p>Even though IOBase does not declare read, readinto, or write because their signatures will vary, implementations and clients should consider those methods part of the interface. Also, implementations may raise UnsupportedOperation when operations they do not support are called. </p> <p>The basic type used for binary data read from or written to a file is bytes. Other bytes-like objects are accepted as method arguments too. In some cases (such as readinto), a writable object is required. Text I/O classes work with str data. </p> <p>Note that calling any method (except additional calls to close(), which are ignored) on a closed stream should raise a ValueError. </p> <p>IOBase (and its subclasses) support the iterator protocol, meaning that an IOBase object can be iterated over yielding the lines in a stream. </p> <p>IOBase also supports the :keyword:<code>with</code> statement. In this example, fp is closed after the suite of the with statement is complete: </p> <p>with open('spam.txt', 'r') as fp:  fp.write('Spam and eggs!') </p> <p></p>"},{"location":"api/io/#class-rawiobase","title":"class <code>RawIOBase</code>","text":"<p>Base class for raw binary I/O. </p> <p></p>"},{"location":"api/io/#class-textiobase","title":"class <code>TextIOBase</code>","text":"<p>Base class for text I/O. </p> <p>This class provides a character and line based interface to stream I/O. There is no readinto method because Python's character strings are immutable. </p> <p></p>"},{"location":"api/io/#class-unsupportedoperation","title":"class <code>UnsupportedOperation</code>","text":"<p>This file was automatically generated via lazydocs.</p>"},{"location":"api/models.bmDCA/","title":"models.bmDCA","text":""},{"location":"api/models.bmDCA/#module-modelsbmdca","title":"module <code>models.bmDCA</code>","text":""},{"location":"api/models.bmDCA/#function-fit","title":"function <code>fit</code>","text":"<pre><code>fit(\n    sampler: Callable,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    chains: Tensor,\n    log_weights: Tensor,\n    target_pearson: float,\n    nsweeps: int,\n    nepochs: int,\n    lr: float,\n    fi_test: Tensor | None = None,\n    fij_test: Tensor | None = None,\n    checkpoint: Checkpoint | None = None,\n    *args,\n    **kwargs\n) \u2192 None\n</code></pre> <p>Trains a bmDCA model on the input MSA and saves the results in a file. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function to be used. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Initialization of the model's parameters. </li> <li><code>mask</code> (torch.Tensor):  Initialization of the coupling matrix's mask. </li> <li><code>chains</code> (torch.Tensor):  Initialization of the Markov chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log-weights of the chains. Used to estimate the log-likelihood. </li> <li><code>target_pearson</code> (float):  Pearson correlation coefficient on the two-points statistics to be reached. </li> <li><code>nsweeps</code> (int):  Number of Monte Carlo steps to update the state of the model. </li> <li><code>nepochs</code> (int):  Maximum number of epochs to be performed. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>fi_test</code> (torch.Tensor | None, optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (torch.Tensor | None, optional):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Checkpoint | None, optional):  Checkpoint class to be used to save the model. Defaults to None. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/models.eaDCA/","title":"models.eaDCA","text":""},{"location":"api/models.eaDCA/#module-modelseadca","title":"module <code>models.eaDCA</code>","text":""},{"location":"api/models.eaDCA/#function-fit","title":"function <code>fit</code>","text":"<pre><code>fit(\n    sampler: Callable,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: dict,\n    mask: Tensor,\n    chains: Tensor,\n    log_weights: Tensor,\n    target_pearson: float,\n    nsweeps: int,\n    nepochs: int,\n    pseudo_count: float,\n    lr: float,\n    factivate: float,\n    gsteps: int,\n    fi_test: Tensor | None = None,\n    fij_test: Tensor | None = None,\n    checkpoint: Checkpoint | None = None,\n    *args,\n    **kwargs\n) \u2192 None\n</code></pre> <p>Fits an eaDCA model on the training data and saves the results in a file. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function to be used. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (dict):  Initialization of the model's parameters. </li> <li><code>mask</code> (torch.Tensor):  Initialization of the coupling matrix's mask. </li> <li><code>chains</code> (torch.Tensor):  Initialization of the Markov chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log-weights of the chains. Used to estimate the log-likelihood. </li> <li><code>target_pearson</code> (float):  Pearson correlation coefficient on the two-points statistics to be reached. </li> <li><code>nsweeps</code> (int):  Number of Monte Carlo steps to update the state of the model. </li> <li><code>nepochs</code> (int):  Maximum number of epochs to be performed. Defaults to 50000. </li> <li><code>pseudo_count</code> (float):  Pseudo count for the single and two points statistics. Acts as a regularization. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>factivate</code> (float):  Fraction of inactive couplings to activate at each step. </li> <li><code>gsteps</code> (int):  Number of gradient updates to be performed on a given graph. </li> <li><code>fi_test</code> (torch.Tensor | None):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (torch.Tensor | None):  Two-point frequencies of the test data. Defaults to None. </li> <li><code>checkpoint</code> (Checkpoint | None):  Checkpoint class to be used to save the model. Defaults to None. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/models.edDCA/","title":"models.edDCA","text":""},{"location":"api/models.edDCA/#module-modelseddca","title":"module <code>models.edDCA</code>","text":""},{"location":"api/models.edDCA/#global-variables","title":"Global Variables","text":"<ul> <li>MAX_EPOCHS</li> </ul>"},{"location":"api/models.edDCA/#function-fit","title":"function <code>fit</code>","text":"<pre><code>fit(\n    sampler: Callable,\n    chains: Tensor,\n    log_weights: Tensor,\n    fi_target: Tensor,\n    fij_target: Tensor,\n    params: Dict[str, Tensor],\n    mask: Tensor,\n    lr: float,\n    nsweeps: int,\n    target_pearson: float,\n    target_density: float,\n    drate: float,\n    checkpoint: Checkpoint,\n    fi_test: Tensor | None = None,\n    fij_test: Tensor | None = None,\n    *args,\n    **kwargs\n)\n</code></pre> <p>Fits an edDCA model on the training data and saves the results in a file. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function to be used. </li> <li><code>chains</code> (torch.Tensor):  Initialization of the Markov chains. </li> <li><code>log_weights</code> (torch.Tensor):  Log-weights of the chains. Used to estimate the log-likelihood. </li> <li><code>fi_target</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>fij_target</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Initialization of the model's parameters. </li> <li><code>mask</code> (torch.Tensor):  Initialization of the coupling matrix's mask. </li> <li><code>lr</code> (float):  Learning rate. </li> <li><code>nsweeps</code> (int):  Number of Monte Carlo steps to update the state of the model. </li> <li><code>target_pearson</code> (float):  Pearson correlation coefficient on the two-points statistics to be reached. </li> <li><code>target_density</code> (float):  Target density of the coupling matrix. </li> <li><code>drate</code> (float):  Percentage of active couplings to be pruned at each decimation step. </li> <li><code>checkpoint</code> (Checkpoint):  Checkpoint class to be used to save the model. </li> <li><code>fi_test</code> (torch.Tensor | None, optional):  Single-point frequencies of the test data. Defaults to None. </li> <li><code>fij_test</code> (torch.Tensor | None, optional):  Two-point frequencies of the test data. Defaults to None. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/models/","title":"Models","text":""},{"location":"api/models/#module-models","title":"module <code>models</code>","text":"<p>This file was automatically generated via lazydocs.</p>"},{"location":"api/parser/","title":"Parser","text":""},{"location":"api/parser/#module-parser","title":"module <code>parser</code>","text":""},{"location":"api/parser/#function-add_args_dca","title":"function <code>add_args_dca</code>","text":"<pre><code>add_args_dca(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_reweighting","title":"function <code>add_args_reweighting</code>","text":"<pre><code>add_args_reweighting(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_eadca","title":"function <code>add_args_eaDCA</code>","text":"<pre><code>add_args_eaDCA(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_eddca","title":"function <code>add_args_edDCA</code>","text":"<pre><code>add_args_edDCA(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_train","title":"function <code>add_args_train</code>","text":"<pre><code>add_args_train(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_energies","title":"function <code>add_args_energies</code>","text":"<pre><code>add_args_energies(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_contacts","title":"function <code>add_args_contacts</code>","text":"<pre><code>add_args_contacts(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_dms","title":"function <code>add_args_dms</code>","text":"<pre><code>add_args_dms(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_sample","title":"function <code>add_args_sample</code>","text":"<pre><code>add_args_sample(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_tdint","title":"function <code>add_args_tdint</code>","text":"<pre><code>add_args_tdint(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_reintegration","title":"function <code>add_args_reintegration</code>","text":"<pre><code>add_args_reintegration(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre>"},{"location":"api/parser/#function-add_args_profmark","title":"function <code>add_args_profmark</code>","text":"<pre><code>add_args_profmark(parser: ArgumentParser) \u2192 ArgumentParser\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/plot/","title":"Helper functions for plotting","text":""},{"location":"api/plot/#module-plot","title":"module <code>plot</code>","text":""},{"location":"api/plot/#function-plot_pca","title":"function <code>plot_PCA</code>","text":"<pre><code>plot_PCA(\n    fig: Figure,\n    data1: ndarray,\n    pc1: int = 0,\n    pc2: int = 1,\n    data2: ndarray | None = None,\n    labels: Union[List[str], str] = 'Data',\n    colors: Union[List[str], str] = 'black',\n    title: str | None = None\n) \u2192 Figure\n</code></pre> <p>Makes the scatter plot of the components (pc1, pc2) of the input data and shows the histograms of the components. </p> <p>Args:</p> <ul> <li><code>fig</code> (plt.figure):  Figure to plot the data. </li> <li><code>data1</code> (np.ndarray):  Data to plot. </li> <li><code>pc1</code> (int, optional):  First principal direction. Defaults to 0. </li> <li><code>pc2</code> (int, optional):  Second principal direction. Defaults to 1. </li> <li><code>data2</code> (np.ndarray | None, optional):  Data to be superimposed to data1. Defaults to None. </li> <li><code>labels</code> (List[str] | str, optional):  Labels to put in the legend. Defaults to \"Data\". </li> <li><code>colors</code> (List[str] | str, optional):  Colors to be used. Defaults to \"black\". </li> <li><code>title</code> (str | None, optional):  Title of the plot. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Figure</code>:  Updated figure. </li> </ul> <p></p>"},{"location":"api/plot/#function-plot_pearson_sampling","title":"function <code>plot_pearson_sampling</code>","text":"<pre><code>plot_pearson_sampling(\n    ax: Axes,\n    checkpoints: ndarray,\n    pearsons: ndarray,\n    pearson_training: ndarray | None = None\n)\n</code></pre> <p>Plots the Pearson correlation coefficient over sampling time. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the data. </li> <li><code>checkpoints</code> (np.ndarray):  Checkpoints of the sampling. </li> <li><code>pearsons</code> (np.ndarray):  Pearson correlation coefficients at different checkpoints. </li> <li><code>pearson_training</code> (np.ndarray | None, optional):  Pearson correlation coefficient obtained during training. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/plot/#function-plot_autocorrelation","title":"function <code>plot_autocorrelation</code>","text":"<pre><code>plot_autocorrelation(\n    ax: Axes,\n    checkpoints: ndarray,\n    autocorr: ndarray,\n    gen_seqid: float,\n    data_seqid: float\n) \u2192 Axes\n</code></pre> <p>Plots the time-autocorrelation curve of the sequence identity and the generated and data sequence identities. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the data. </li> <li><code>checkpoints</code> (np.ndarray):  Checkpoints of the sampling. </li> <li><code>autocorr</code> (np.ndarray):  Time-autocorrelation of the sequence identity. </li> <li><code>gen_seqid</code> (float):  Sequence identity of the generated data. </li> <li><code>data_seqid</code> (float):  Sequence identity of the data. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/plot/#function-plot_scatter_correlations","title":"function <code>plot_scatter_correlations</code>","text":"<pre><code>plot_scatter_correlations(\n    ax: Tuple[Axes, Axes],\n    Cij_data: ndarray,\n    Cij_gen: ndarray,\n    Cijk_data: ndarray,\n    Cijk_gen: ndarray,\n    pearson_Cij: float,\n    pearson_Cijk: float\n) \u2192 Axes\n</code></pre> <p>Plots the scatter plot of the data and generated Cij and Cijk values. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the data. Must have 2 subplots. </li> <li><code>Cij_data</code> (np.ndarray):  Data Cij values. </li> <li><code>Cij_gen</code> (np.ndarray):  Generated Cij values. </li> <li><code>Cijk_data</code> (np.ndarray):  Data Cijk values. </li> <li><code>Cijk_gen</code> (np.ndarray):  Generated Cijk values. </li> <li><code>pearson_Cij</code> (float):  Pearson correlation coefficient of Cij. </li> <li><code>pearson_Cijk</code> (float):  Pearson correlation coefficient of Cijk. </li> </ul> <p>Returns:</p> <ul> <li><code>plt.Axes</code>:  Updated axes. </li> </ul> <p></p>"},{"location":"api/plot/#function-plot_contact_map","title":"function <code>plot_contact_map</code>","text":"<pre><code>plot_contact_map(ax: Axes, cm: ndarray, title: str | None = None) \u2192 Axes\n</code></pre> <p>Plots the contact map. </p> <p>Args:</p> <ul> <li><code>ax</code> (Axes):  Axes to plot the contact map. </li> <li><code>cm</code> (np.ndarray):  Contact map to plot. </li> <li><code>title</code> (str | None, optional):  Title of the plot. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Axes</code>:  Updated axes. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/resampling/","title":"Resampling","text":""},{"location":"api/resampling/#module-resampling","title":"module <code>resampling</code>","text":""},{"location":"api/resampling/#function-compute_mixing_time","title":"function <code>compute_mixing_time</code>","text":"<pre><code>compute_mixing_time(\n    sampler: Callable,\n    data: Tensor,\n    params: Dict[str, Tensor],\n    n_max_sweeps: int,\n    beta: float\n) \u2192 Dict[str, list]\n</code></pre> <p>Computes the mixing time using the t and t/2 method. The sampling will halt when the mixing time is reached or the limit of <code>n_max_sweeps</code> sweeps is reached. </p> <p>Args:</p> <ul> <li><code>sampler</code> (Callable):  Sampling function. </li> <li><code>data</code> (torch.Tensor):  Initial data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters for the sampling. </li> <li><code>n_max_sweeps</code> (int):  Maximum number of sweeps. </li> <li><code>beta</code> (float):  Inverse temperature for the sampling. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, list]</code>:  Results of the mixing time analysis. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/sampling/","title":"Sampling functions","text":""},{"location":"api/sampling/#module-sampling","title":"module <code>sampling</code>","text":""},{"location":"api/sampling/#function-gibbs_step_uniform_sites","title":"function <code>gibbs_step_uniform_sites</code>","text":"<pre><code>gibbs_step_uniform_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Gibbs sampler. In this version, the mutation is attempted at the same sites for all chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>beta</code> (float):  Inverse temperature. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-gibbs_step_independent_sites","title":"function <code>gibbs_step_independent_sites</code>","text":"<pre><code>gibbs_step_independent_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Gibbs sampler. This version selects different random sites for each chain. It is less efficient than the 'gibbs_step_uniform_sites' function, but it is more suitable for mutating staring from the same wild-type sequence since mutations are independent across chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>beta</code> (float):  Inverse temperature. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-gibbs_sampling","title":"function <code>gibbs_sampling</code>","text":"<pre><code>gibbs_sampling(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Gibbs sampling. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  Initial one-hot encoded chains of size (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>nsweeps</code> (int):  Number of sweeps, where one sweep corresponds to attempting L mutations. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-metropolis_step_uniform_sites","title":"function <code>metropolis_step_uniform_sites</code>","text":"<pre><code>metropolis_step_uniform_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Metropolis sampler. In this version, the mutation is attempted at the same sites for all chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-metropolis_step_independent_sites","title":"function <code>metropolis_step_independent_sites</code>","text":"<pre><code>metropolis_step_independent_sites(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Performs a single mutation using the Metropolis sampler. This version selects different random sites for each chain. It is less efficient than the 'metropolis_step_uniform_sites' function, but it is more suitable for mutating staring from the same wild-type sequence since mutations are independent across chains. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-metropolis_sampling","title":"function <code>metropolis_sampling</code>","text":"<pre><code>metropolis_sampling(\n    chains: Tensor,\n    params: Dict[str, Tensor],\n    nsweeps: int,\n    beta: float = 1.0\n) \u2192 Tensor\n</code></pre> <p>Metropolis sampling. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  One-hot encoded sequences of shape (batch_size, L, q). </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>nsweeps</code> (int):  Number of sweeps to be performed, where one sweep corresponds to attempting L mutations. </li> <li><code>beta</code> (float, optional):  Inverse temperature. Defaults to 1.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Updated chains. </li> </ul> <p></p>"},{"location":"api/sampling/#function-get_sampler","title":"function <code>get_sampler</code>","text":"<pre><code>get_sampler(sampling_method: str) \u2192 Callable\n</code></pre> <p>Returns the sampling function corresponding to the chosen method. </p> <p>Args:</p> <ul> <li><code>sampling_method</code> (str):  String indicating the sampling method. Choose between 'metropolis' and 'gibbs'. </li> </ul> <p>Raises:</p> <ul> <li><code>KeyError</code>:  Unknown sampling method. </li> </ul> <p>Returns:</p> <ul> <li><code>Callable</code>:  Sampling function. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.contacts/","title":"Scripts.contacts","text":""},{"location":"api/scripts.contacts/#module-scriptscontacts","title":"module <code>scripts.contacts</code>","text":""},{"location":"api/scripts.contacts/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.contacts/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.convert_format_params/","title":"Scripts.convert format params","text":""},{"location":"api/scripts.convert_format_params/#module-scriptsconvert_format_params","title":"module <code>scripts.convert_format_params</code>","text":""},{"location":"api/scripts.convert_format_params/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.convert_format_params/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.dms/","title":"Scripts.dms","text":""},{"location":"api/scripts.dms/#module-scriptsdms","title":"module <code>scripts.dms</code>","text":""},{"location":"api/scripts.dms/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.dms/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.energies/","title":"Scripts.energies","text":""},{"location":"api/scripts.energies/#module-scriptsenergies","title":"module <code>scripts.energies</code>","text":""},{"location":"api/scripts.energies/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.energies/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts/","title":"Scripts","text":""},{"location":"api/scripts/#module-scripts","title":"module <code>scripts</code>","text":"<p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.profmark/","title":"Scripts.profmark","text":""},{"location":"api/scripts.profmark/#module-scriptsprofmark","title":"module <code>scripts.profmark</code>","text":""},{"location":"api/scripts.profmark/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.profmark/#function-main","title":"function <code>main</code>","text":"<pre><code>main(args)\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.reintegrate/","title":"Scripts.reintegrate","text":""},{"location":"api/scripts.reintegrate/#module-scriptsreintegrate","title":"module <code>scripts.reintegrate</code>","text":""},{"location":"api/scripts.reintegrate/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.reintegrate/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.sample/","title":"Scripts.sample","text":""},{"location":"api/scripts.sample/#module-scriptssample","title":"module <code>scripts.sample</code>","text":""},{"location":"api/scripts.sample/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.sample/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.td_integration/","title":"Scripts.td integration","text":""},{"location":"api/scripts.td_integration/#module-scriptstd_integration","title":"module <code>scripts.td_integration</code>","text":""},{"location":"api/scripts.td_integration/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.td_integration/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/scripts.train/","title":"Scripts.train","text":""},{"location":"api/scripts.train/#module-scriptstrain","title":"module <code>scripts.train</code>","text":""},{"location":"api/scripts.train/#function-create_parser","title":"function <code>create_parser</code>","text":"<pre><code>create_parser()\n</code></pre>"},{"location":"api/scripts.train/#function-main","title":"function <code>main</code>","text":"<pre><code>main()\n</code></pre> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/statmech/","title":"Statistical mechanics functions","text":""},{"location":"api/statmech/#module-statmech","title":"module <code>statmech</code>","text":""},{"location":"api/statmech/#function-compute_energy","title":"function <code>compute_energy</code>","text":"<pre><code>compute_energy(X: Tensor, params: Dict[str, Tensor]) \u2192 Tensor\n</code></pre> <p>Compute the DCA energy of the sequences in X. </p> <p>Args:</p> <ul> <li><code>X</code> (torch.Tensor):  Sequences in one-hot encoding format. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  DCA Energy of the sequences. </li> </ul> <p></p>"},{"location":"api/statmech/#function-compute_log_likelihood","title":"function <code>compute_log_likelihood</code>","text":"<pre><code>compute_log_likelihood(\n    fi: Tensor,\n    fij: Tensor,\n    params: Dict[str, Tensor],\n    logZ: float\n) \u2192 float\n</code></pre> <p>Compute the log-likelihood of the model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-site frequencies of the data. </li> <li><code>fij</code> (torch.Tensor):  Two-site frequencies of the data. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>logZ</code> (float):  Log-partition function of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Log-likelihood of the model. </li> </ul> <p></p>"},{"location":"api/statmech/#function-enumerate_states","title":"function <code>enumerate_states</code>","text":"<pre><code>enumerate_states(L: int, q: int, device: device = device(type='cpu')) \u2192 Tensor\n</code></pre> <p>Enumerate all possible states of a system of L sites and q states. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Number of sites. </li> <li><code>q</code> (int):  Number of states. </li> <li><code>device</code> (torch.device, optional):  Device to store the states. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  All possible states. </li> </ul> <p></p>"},{"location":"api/statmech/#function-compute_logz_exact","title":"function <code>compute_logZ_exact</code>","text":"<pre><code>compute_logZ_exact(all_states: Tensor, params: Dict[str, Tensor]) \u2192 float\n</code></pre> <p>Compute the log-partition function of the model. </p> <p>Args:</p> <ul> <li><code>all_states</code> (torch.Tensor):  All possible states of the system. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Log-partition function of the model. </li> </ul> <p></p>"},{"location":"api/statmech/#function-compute_entropy","title":"function <code>compute_entropy</code>","text":"<pre><code>compute_entropy(chains: Tensor, params: Dict[str, Tensor], logZ: float) \u2192 float\n</code></pre> <p>Compute the entropy of the DCA model. </p> <p>Args:</p> <ul> <li><code>chains</code> (torch.Tensor):  Chains that are supposed to be an equilibrium realization of the model. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>logZ</code> (float):  Log-partition function of the model. </li> </ul> <p>Returns:</p> <ul> <li><code>float</code>:  Entropy of the model. </li> </ul> <p></p>"},{"location":"api/statmech/#function-iterate_tap","title":"function <code>iterate_tap</code>","text":"<pre><code>iterate_tap(\n    mag: Tensor,\n    params: Dict[str, Tensor],\n    max_iter: int = 500,\n    epsilon: float = 0.0001\n)\n</code></pre> <p>Iterates the TAP equations until convergence. </p> <p>Args:</p> <ul> <li><code>mag</code> (torch.Tensor):  Initial magnetizations. </li> <li><code>params</code> (Dict[str, torch.Tensor]):  Parameters of the model. </li> <li><code>max_iter</code> (int, optional):  Maximum number of iterations. Defaults to 2000. </li> <li><code>epsilon</code> (float, optional):  Convergence threshold. Defaults to 1e-6. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Fixed point magnetizations of the TAP equations. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/stats/","title":"Compute statistics","text":""},{"location":"api/stats/#module-stats","title":"module <code>stats</code>","text":""},{"location":"api/stats/#function-get_freq_single_point","title":"function <code>get_freq_single_point</code>","text":"<pre><code>get_freq_single_point(\n    data: Tensor,\n    weights: Tensor | None = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the single point frequencies of the input MSA. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  One-hot encoded data array. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Weights of the sequences. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count to be added to the frequencies. Defaults to 0.0. </li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>:  If the input data is not a 3D tensor. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Single point frequencies. </li> </ul> <p></p>"},{"location":"api/stats/#function-get_freq_two_points","title":"function <code>get_freq_two_points</code>","text":"<pre><code>get_freq_two_points(\n    data: Tensor,\n    weights: Tensor | None = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the 2-points statistics of the input MSA. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  One-hot encoded data array. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Array of weights to assign to the sequences of shape. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count for the single and two points statistics. Acts as a regularization. Defaults to 0.0. </li> </ul> <p>Raises:</p> <ul> <li><code>ValueError</code>:  If the input data is not a 3D tensor. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Matrix of two-point frequencies of shape (L, q, L, q). </li> </ul> <p></p>"},{"location":"api/stats/#function-generate_unique_triplets","title":"function <code>generate_unique_triplets</code>","text":"<pre><code>generate_unique_triplets(\n    L: int,\n    ntriplets: int,\n    device: device = device(type='cpu')\n) \u2192 Tensor\n</code></pre> <p>Generates a set of unique triplets of positions. Used to compute the 3-points statistics. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Length of the sequences. </li> <li><code>ntriplets</code> (int):  Number of triplets to be generated. </li> <li><code>device</code> (torch.device, optional):  Device to perform computations on. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Tensor of shape (ntriplets, 3) containing the indices of the triplets. </li> </ul> <p></p>"},{"location":"api/stats/#function-get_freq_three_points","title":"function <code>get_freq_three_points</code>","text":"<pre><code>get_freq_three_points(\n    nat: Tensor,\n    gen: Tensor,\n    ntriplets: int,\n    weights: Tensor | None = None,\n    device: device = device(type='cpu')\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Computes the 3-body connected correlation statistics of the input MSAs. </p> <p>Args:</p> <ul> <li><code>nat</code> (torch.Tensor):  Input MSA representing natural data in one-hot encoding. </li> <li><code>gen</code> (torch.Tensor):  Input MSA representing generated data in one-hot encoding. </li> <li><code>ntriplets</code> (int):  Number of triplets to test. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Importance weights for the natural sequences. Defaults to None. </li> <li><code>device</code> (torch.device, optional):  Device to perform computations on. Defaults to \"cpu\". </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Natural and generated 3-points connected correlation for ntriplets randomly extracted triplets. </li> </ul> <p></p>"},{"location":"api/stats/#function-get_covariance_matrix","title":"function <code>get_covariance_matrix</code>","text":"<pre><code>get_covariance_matrix(\n    data: Tensor,\n    weights: Tensor | None = None,\n    pseudo_count: float = 0.0\n) \u2192 Tensor\n</code></pre> <p>Computes the weighted covariance matrix of the input multi sequence alignment. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Input MSA in one-hot variables. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Importance weights of the sequences. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count. Defaults to 0.0. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Covariance matrix. </li> </ul> <p></p>"},{"location":"api/stats/#function-extract_cij_from_freq","title":"function <code>extract_Cij_from_freq</code>","text":"<pre><code>extract_Cij_from_freq(\n    fij: Tensor,\n    pij: Tensor,\n    fi: Tensor,\n    pi: Tensor,\n    mask: Tensor | None = None\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Extracts the lower triangular part of the covariance matrices of the data and chains starting from the frequencies. </p> <p>Args:</p> <ul> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>pij</code> (torch.Tensor):  Two-point frequencies of the chains. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>pi</code> (torch.Tensor):  Single-point frequencies of the chains. </li> <li><code>mask</code> (torch.Tensor | None, optional):  Mask for comparing just a subset of the couplings. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Extracted two-point frequencies of the data and chains. </li> </ul> <p></p>"},{"location":"api/stats/#function-extract_cij_from_seqs","title":"function <code>extract_Cij_from_seqs</code>","text":"<pre><code>extract_Cij_from_seqs(\n    data: Tensor,\n    chains: Tensor,\n    weights: Tensor | None = None,\n    pseudo_count: float = 0.0,\n    mask: Tensor | None = None\n) \u2192 Tuple[Tensor, Tensor]\n</code></pre> <p>Extracts the lower triangular part of the covariance matrices of the data and chains starting from the sequences. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Data sequences. </li> <li><code>chains</code> (torch.Tensor):  Chain sequences. </li> <li><code>weights</code> (torch.Tensor | None, optional):  Weights of the sequences. Defaults to None. </li> <li><code>pseudo_count</code> (float, optional):  Pseudo count for the single and two points statistics. Acts as a regularization. Defaults to 0.0. </li> <li><code>mask</code> (torch.Tensor | None, optional):  Mask for comparing just a subset of the couplings. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[torch.Tensor, torch.Tensor]</code>:  Two-point frequencies of the data and chains. </li> </ul> <p></p>"},{"location":"api/stats/#function-get_correlation_two_points","title":"function <code>get_correlation_two_points</code>","text":"<pre><code>get_correlation_two_points(\n    fij: Tensor,\n    pij: Tensor,\n    fi: Tensor,\n    pi: Tensor,\n    mask: Tensor | None = None\n) \u2192 Tuple[float, float]\n</code></pre> <p>Computes the Pearson coefficient and the slope between the two-point frequencies of data and chains. </p> <p>Args:</p> <ul> <li><code>fij</code> (torch.Tensor):  Two-point frequencies of the data. </li> <li><code>pij</code> (torch.Tensor):  Two-point frequencies of the chains. </li> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> <li><code>pi</code> (torch.Tensor):  Single-point frequencies of the chains. </li> <li><code>mask</code> (torch.Tensor | None, optional):  Mask to select the couplings to use for the correlation coefficient. Defaults to None.  </li> </ul> <p>Returns:</p> <ul> <li><code>Tuple[float, float]</code>:  Pearson correlation coefficient of the two-sites statistics and slope of the interpolating line. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"},{"location":"api/utils/","title":"Utilities","text":""},{"location":"api/utils/#module-utils","title":"module <code>utils</code>","text":""},{"location":"api/utils/#function-init_parameters","title":"function <code>init_parameters</code>","text":"<pre><code>init_parameters(fi: Tensor) \u2192 Dict[str, Tensor]\n</code></pre> <p>Initialize the parameters of the DCA model. </p> <p>Args:</p> <ul> <li><code>fi</code> (torch.Tensor):  Single-point frequencies of the data. </li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, torch.Tensor]</code>:  Parameters of the model. </li> </ul> <p></p>"},{"location":"api/utils/#function-init_chains","title":"function <code>init_chains</code>","text":"<pre><code>init_chains(\n    num_chains: int,\n    L: int,\n    q: int,\n    device: device,\n    dtype: dtype = torch.float32,\n    fi: Tensor | None = None\n) \u2192 Tensor\n</code></pre> <p>Initialize the chains of the DCA model. If 'fi' is provided, the chains are sampled from the profile model, otherwise they are sampled uniformly at random. </p> <p>Args:</p> <ul> <li><code>num_chains</code> (int):  Number of parallel chains. </li> <li><code>L</code> (int):  Length of the MSA. </li> <li><code>q</code> (int):  Number of values that each residue can assume. </li> <li><code>device</code> (torch.device):  Device where to store the chains. </li> <li><code>dtype</code> (torch.dtype, optional):  Data type of the chains. Defaults to torch.float32. </li> <li><code>fi</code> (torch.Tensor | None, optional):  Single-point frequencies. Defaults to None. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Initialized parallel chains in one-hot encoding format. </li> </ul> <p></p>"},{"location":"api/utils/#function-get_mask_save","title":"function <code>get_mask_save</code>","text":"<pre><code>get_mask_save(L: int, q: int, device: device) \u2192 Tensor\n</code></pre> <p>Returns the mask to save the upper-triangular part of the coupling matrix. </p> <p>Args:</p> <ul> <li><code>L</code> (int):  Length of the MSA. </li> <li><code>q</code> (int):  Number of values that each residue can assume. </li> <li><code>device</code> (torch.device):  Device where to store the mask. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Mask. </li> </ul> <p></p>"},{"location":"api/utils/#function-resample_sequences","title":"function <code>resample_sequences</code>","text":"<pre><code>resample_sequences(data: Tensor, weights: Tensor, nextract: int) \u2192 Tensor\n</code></pre> <p>Extracts nextract sequences from data with replacement according to the weights. </p> <p>Args:</p> <ul> <li><code>data</code> (torch.Tensor):  Data array. </li> <li><code>weights</code> (torch.Tensor):  Weights of the sequences. </li> <li><code>nextract</code> (int):  Number of sequences to be extracted. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.Tensor</code>:  Extracted sequences. </li> </ul> <p></p>"},{"location":"api/utils/#function-get_device","title":"function <code>get_device</code>","text":"<pre><code>get_device(device: str, message: bool = True) \u2192 device\n</code></pre> <p>Returns the device where to store the tensors. </p> <p>Args:</p> <ul> <li><code>device</code> (str):  Device to be used. </li> <li><code>message</code> (bool, optional):  Print the device. Defaults to True. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.device</code>:  Device. </li> </ul> <p></p>"},{"location":"api/utils/#function-get_dtype","title":"function <code>get_dtype</code>","text":"<pre><code>get_dtype(dtype: str) \u2192 dtype\n</code></pre> <p>Returns the data type of the tensors. </p> <p>Args:</p> <ul> <li><code>dtype</code> (str):  Data type. </li> </ul> <p>Returns:</p> <ul> <li><code>torch.dtype</code>:  Data type. </li> </ul> <p>This file was automatically generated via lazydocs.</p>"}]}